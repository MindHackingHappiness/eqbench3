
# File: ai/eqbench3/core/benchmark.py

# core/benchmark.py

import os
import re
import uuid
import time
import logging
import json # For constructing rubric output format
from datetime import datetime, timezone
from typing import Dict, List, Optional, Any, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
import queue
import threading
import statistics # For averaging rubric scores
from pathlib import Path

from ..utils.file_io import load_json_file, update_run_data, save_json_file
from ..utils.api import APIClient
from .conversation import ScenarioTask
from .elo import run_elo_analysis_eqbench3 # Keep existing import
# Import constants including file paths and scenario type IDs
import eqbench3.utils.constants as C
from collections import defaultdict

ALLOW_INCOMPLETE_RESPONSES = True

# --- Helper Function for the Save Worker Thread ---
def _save_worker(save_queue: queue.Queue, local_runs_file: str, batch_size: int = 10):
    """
    Save‑worker thread. Writes ONLY to the local runs file.
    Accumulates `batch_size` queue items before writing to disk, to reduce I/O.
    If a sentinel (None) is received, any remaining queued items are flushed
    immediately before the thread exits.

    Args:
        save_queue (queue.Queue):   Queue populated by producer threads.
        local_runs_file (str):      Path to the LOCAL JSON file holding run data.
        batch_size (int, optional): Number of tasks to buffer before saving. Defaults to 10.
    """
    logging.info(f"[SaveWorker] Save worker thread started. Target file: {local_runs_file}")

    # -----------------------------  helper: flush_batch  ----------------------------- #
    def flush_batch(item_batch: list):
        """
        Write all queued items in `item_batch` to the local runs file, grouped by run_key.
        Clears `item_batch` on completion.
        """
        if not item_batch:
            return

        # Group pending updates by run_key to minimise file operations
        grouped_updates: dict[str, dict] = defaultdict(lambda: {"scenario_tasks": {}})

        for run_key, iteration_index, scenario_id, task_data in item_batch:
            iter_dict = grouped_updates[run_key]["scenario_tasks"].setdefault(str(iteration_index), {})
            iter_dict[str(scenario_id)] = task_data

        for run_key, update_dict in grouped_updates.items():
            # Ensure writing ONLY to the local file
            ok = update_run_data(local_runs_file, run_key, update_dict, max_retries=5, retry_delay=0.75)
            if ok:
                logging.debug(
                    f"[SaveWorker] Flushed {len(item_batch)} tasks for run {run_key} to {local_runs_file}."
                )
            else:
                logging.error(f"[SaveWorker] Failed to flush batch for run {run_key} to {local_runs_file}.")

        item_batch.clear()
    # ------------------------------------------------------------------------------- #

    pending_items: list[tuple[str, int, int, dict]] = []

    while True:
        try:
            item = save_queue.get()  # block until an item arrives

            # Sentinel => flush anything buffered, then exit
            if item is None:
                logging.info("[SaveWorker] Sentinel received. Flushing remaining tasks.")
                flush_batch(pending_items)
                save_queue.task_done()
                break

            pending_items.append(item)

            # If buffer full, write to disk
            if len(pending_items) >= batch_size:
                flush_batch(pending_items)

            save_queue.task_done()

        except Exception as e:
            logging.error("[SaveWorker] Error handling queue item: %s", e, exc_info=True)
            try:
                save_queue.task_done()
            except ValueError:
                pass  # task_done() called too many times

    logging.info("[SaveWorker] Save worker thread finished.")



# --- (Keep parse_scenario_prompts as is) ---
def parse_scenario_prompts(file_path: str) -> Dict[str, List[str]]:
    """Parses the scenario prompts file into a dictionary (Revised Logic)."""
    scenarios: Dict[str, List[str]] = {}
    current_scenario_id: Optional[str] = None
    current_prompts_for_scenario: List[str] = []
    current_prompt_lines: List[str] = []
    in_prompt_content = False # Flag to indicate if we are currently reading lines for a prompt

    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line_num, raw_line in enumerate(f, 1):
                line = raw_line.strip()

                # Check for delimiters first
                scenario_match = re.match(r'^########\s*(\S+)', line)
                prompt_match = re.match(r'^#######\s*Prompt(\d+)', line)

                # --- Handle Scenario Start ---
                if scenario_match:
                    # 1. Finalize the last prompt of the previous scenario (if any)
                    if current_prompt_lines:
                        prompt_text = "\n".join(current_prompt_lines).strip()
                        if prompt_text:
                            current_prompts_for_scenario.append(prompt_text)
                        current_prompt_lines = [] # Reset for the new scenario

                    # 2. Store the completed previous scenario (if any)
                    if current_scenario_id and current_prompts_for_scenario:
                        scenarios[current_scenario_id] = current_prompts_for_scenario
                        logging.debug(f"Stored scenario {current_scenario_id} with {len(current_prompts_for_scenario)} prompts.")

                    # 3. Start the new scenario
                    current_scenario_id = scenario_match.group(1)
                    current_prompts_for_scenario = []
                    in_prompt_content = False # Reset flag, wait for a Prompt delimiter
                    logging.debug(f"Starting parse for scenario {current_scenario_id} (Line {line_num})")
                    continue # Move to next line

                # --- Handle Prompt Start ---
                elif prompt_match:
                    if current_scenario_id is None:
                        logging.warning(f"Line {line_num}: Found prompt delimiter but no active scenario ID: {line}")
                        continue

                    # 1. Finalize the previous prompt within the current scenario (if any)
                    if current_prompt_lines:
                        prompt_text = "\n".join(current_prompt_lines).strip()
                        if prompt_text:
                            current_prompts_for_scenario.append(prompt_text)

                    # 2. Start collecting lines for the new prompt
                    current_prompt_lines = []
                    in_prompt_content = True # Start collecting content lines
                    logging.debug(f"Starting Prompt {prompt_match.group(1)} for scenario {current_scenario_id} (Line {line_num})")
                    continue # Move to next line

                # --- Handle Content Lines ---
                elif current_scenario_id and in_prompt_content:
                    # Append the raw line (preserving leading/trailing whitespace within the prompt)
                    # but skip truly empty lines between prompts if desired (using strip check above)
                    current_prompt_lines.append(raw_line.rstrip('\n\r')) # Keep indentation, remove trailing newline

                # --- Handle other lines (e.g., comments, blank lines between scenarios/prompts) ---
                elif line: # Log unexpected non-empty lines if not collecting content
                     if not current_scenario_id:
                          logging.debug(f"Line {line_num}: Skipping non-empty line before first scenario: {line[:50]}...")
                     elif not in_prompt_content:
                          logging.debug(f"Line {line_num}: Skipping non-empty line before first prompt in scenario {current_scenario_id}: {line[:50]}...")
                     # else: line is content, handled above

            # --- After loop: Finalize the last prompt and scenario ---
            if current_prompt_lines:
                prompt_text = "\n".join(current_prompt_lines).strip()
                if prompt_text:
                    current_prompts_for_scenario.append(prompt_text)

            if current_scenario_id and current_prompts_for_scenario:
                scenarios[current_scenario_id] = current_prompts_for_scenario
                logging.debug(f"Stored final scenario {current_scenario_id} with {len(current_prompts_for_scenario)} prompts.")

    except FileNotFoundError:
        logging.error(f"Scenario prompts file not found: {file_path}")
        raise
    except Exception as e:
        logging.error(f"Error parsing scenario prompts file {file_path}: {e}", exc_info=True)
        raise

    if not scenarios:
         logging.warning(f"Parsing finished, but no scenarios were loaded from {file_path}.")
    else:
         logging.info(f"Successfully parsed {len(scenarios)} scenarios from {file_path}")

    return scenarios


# --- Function to calculate final rubric score ---
def calculate_final_rubric_score(run_data: Dict[str, Any]) -> Tuple[Optional[float], Optional[str]]:
    """
    Calculates the average rubric score across all completed tasks in a run.
    Considers different criteria sets for standard/drafting vs analysis tasks.
    Averages scores for allowed criteria within a task first, then averages these task scores.
    Returns (average_score, error_message).
    """
    # Define the specific criteria to include for standard/drafting tasks
    STANDARD_ALLOWED_RUBRIC_CRITERIA = {
        "demonstrated_empathy",
        "pragmatic_ei",
        "depth_of_insight",
        "social_dexterity",
        "emotional_reasoning",
        "message_tailoring",
        "theory_of_mind",
        "subtext_identification",
        "intellectual_grounding",
        "correctness"
    }
    # Define the specific criteria to include for analysis tasks
    # Assumes analysis criteria are loaded from ANALYSIS_RUBRIC_CRITERIA_FILE
    # We need to load them here or pass them in. Let's load them.
    analysis_criteria = []
    try:
        with open(C.ANALYSIS_RUBRIC_CRITERIA_FILE, 'r', encoding='utf-8') as f:
            analysis_criteria = [line.strip() for line in f if line.strip() and not line.strip().startswith('#')]
        ANALYSIS_ALLOWED_RUBRIC_CRITERIA = set(analysis_criteria)
        if not ANALYSIS_ALLOWED_RUBRIC_CRITERIA:
             logging.warning(f"Analysis rubric criteria file loaded but no criteria found: {C.ANALYSIS_RUBRIC_CRITERIA_FILE}")
    except Exception as e:
        logging.error(f"Failed to load analysis rubric criteria for score calculation: {e}. Analysis tasks will not contribute to score.")
        ANALYSIS_ALLOWED_RUBRIC_CRITERIA = set()


    logging.info(f"Calculating final rubric score using criteria:")
    logging.info(f"  Standard/Drafting: {', '.join(sorted(STANDARD_ALLOWED_RUBRIC_CRITERIA))}")
    logging.info(f"  Analysis: {', '.join(sorted(ANALYSIS_ALLOWED_RUBRIC_CRITERIA))}")

    scenario_tasks_data = run_data.get("scenario_tasks", {})
    task_average_scores = []
    tasks_considered = 0
    tasks_with_scores = 0
    total_valid_scores_considered = 0 # Track scores used in the average
    run_key_for_log = run_data.get("run_key", "UnknownRun") # Get run key for logging

    for iter_str, scenario_dict in scenario_tasks_data.items():
        if not isinstance(scenario_dict, dict): continue
        for sid, task_info in scenario_dict.items():
            if isinstance(task_info, dict) and task_info.get("status") == "rubric_scored":
                tasks_considered += 1
                rubric_scores = task_info.get("rubric_scores")
                is_analysis = sid in C.ANALYSIS_SCENARIO_IDS
                allowed_criteria = ANALYSIS_ALLOWED_RUBRIC_CRITERIA if is_analysis else STANDARD_ALLOWED_RUBRIC_CRITERIA

                if isinstance(rubric_scores, dict) and rubric_scores:
                    # Calculate average score for this task using ONLY allowed criteria for its type
                    valid_task_scores = []
                    for metric, score in rubric_scores.items():
                        if isinstance(score, (int, float)) and metric in allowed_criteria:
                            weight = 3 if metric == "overall_eq" else 1
                            valid_task_scores.extend([score] * weight)


                    if valid_task_scores:
                        task_avg = statistics.mean(valid_task_scores)
                        task_average_scores.append(task_avg)
                        tasks_with_scores += 1
                        total_valid_scores_considered += len(valid_task_scores) # Count only used scores
                    else:
                        logging.warning(f"Task {run_key_for_log}/{iter_str}/{sid} (Analysis: {is_analysis}) has status 'rubric_scored' but no valid numeric scores found for the *allowed* criteria ({', '.join(sorted(allowed_criteria))}).")
                else:
                    logging.warning(f"Task {run_key_for_log}/{iter_str}/{sid} (Analysis: {is_analysis}) has status 'rubric_scored' but 'rubric_scores' is missing or empty.")

    if not task_average_scores:
        if tasks_considered > 0:
            return None, f"No valid rubric scores found for allowed criteria in {tasks_considered} potentially scored tasks."
        else:
            return None, "No tasks reached the 'rubric_scored' state."

    final_average_score = statistics.mean(task_average_scores)
    logging.info(f"Calculated final rubric score: {final_average_score:.2f} (averaged from {tasks_with_scores} tasks with scores, using {total_valid_scores_considered} total score points from allowed criteria).")
    return round(final_average_score, 2), None


# --- Helper function for executing rubric scoring in a thread ---
def _execute_rubric_scoring_task(
    task: ScenarioTask,
    api_clients: Dict[str, APIClient],
    judge_model_id: str,
    # Pass the specific template and format string for this task type
    rubric_prompt_template: str,
    rubric_output_format_str: str,
    save_queue: queue.Queue,
    run_key: str,
    truncate_for_rubric: bool # Keep flag
):
    """Target function for rubric scoring threads."""
    judge_api = api_clients.get("judge")
    if not judge_api:
        logging.error(f"Judge API client not found for task {task.scenario_id} (Iter {task.iteration_index}).")
        task.status = "error"
        task.error = "Rubric Scoring Error: Judge API client missing."
        task.rubric_run_error = "Judge API client missing."
        task._save_progress(save_queue, run_key)
        return

    # Set status and save progress before starting
    task.status = "running_rubric_scoring"
    task.rubric_run_error = None # Clear previous error if retrying
    task.error = None
    task._save_progress(save_queue, run_key)

    try:
        # 1. Prepare the prompt using the task's helper method, passing the truncation flag
        # The helper method now handles different task types (incl. analysis)
        prompt_text = task.prepare_rubric_prompt_text(
            rubric_prompt_template, # Pass the specific template
            rubric_output_format_str, # Pass the specific format string
            truncate_for_rubric # Pass the flag
        )
        prompt_text = prompt_text.replace('*', '').replace('#', '')
        # print(f"--- Rubric Prompt (Task: {task.scenario_id}, Iter: {task.iteration_index}) ---")
        # print(prompt_text)
        # print("--- End Rubric Prompt ---")
        if prompt_text is None: # Handle error during prompt preparation
            # Error is logged and status set within prepare_rubric_prompt_text if it fails
            # Ensure state is saved
            task._save_progress(save_queue, run_key)
            return

        # 2. Call the Judge API
        logging.debug(f"Calling judge API ({judge_model_id}) for rubric scoring: Task {task.scenario_id} (Iter {task.iteration_index})")
        response_text = judge_api.generate(
            model=judge_model_id,
            messages=[{"role": "user", "content": prompt_text}],
            temperature=0.0, # Zero temp for deterministic scoring
            max_tokens=8000, # Enough for scores and reasoning
            min_p=None # No min_p for judge
        )

        # 3. Process the response using the task's helper method
        task.process_rubric_response(response_text.strip())
        # Status (rubric_scored or error), scores, errors are set within process_rubric_response

    except Exception as e:
        # Catch errors during API call or unexpected errors in helpers
        error_msg = f"Rubric Scoring Error: API call failed or processing error: {str(e)}"
        logging.error(f"Error during rubric scoring execution for task {task.scenario_id} (Iter {task.iteration_index}): {e}", exc_info=True)
        task.status = "error"
        task.error = error_msg
        task.rubric_run_error = str(e)
        # Clear potentially partial results from failed processing
        task.rubric_scores = None
        task.raw_rubric_judge_text = None

    finally:
        # 4. Save the final state of the task after this step
        task._save_progress(save_queue, run_key)

# ---------- completeness-check helpers ------------------------------------
_MIN_RAW_LEN = 100

MANDATORY_SECTIONS_ROLEPLAY = {"thinking_feeling",
                               "their_thinking_feeling",
                               "response"}
MANDATORY_SECTIONS_DRAFTING = {"perspective_taking",
                               "draft_brainstorming",
                               "draft"}


def _task_has_all_expected_responses(task: "ScenarioTask") -> bool:
    """
    Decide whether *task* is complete enough for rubric scoring.

    Baseline (even when `ALLOW_INCOMPLETE_RESPONSES` is True):
    ▸ There must be **at least one** assistant message in the main
      conversation (i.e., not the debrief) whose raw text length is
      ≥ `_MIN_RAW_LEN`.

    • NO_RP / Analysis : at least one assistant message ≥ `_MIN_RAW_LEN`
    • Role-play / Draft:
        – every assistant turn must have either
            • raw text ≥ `_MIN_RAW_LEN`  **or**
            • all mandatory parsed sections non-blank
        – plus a non-empty debrief.
    """
    sid = task.scenario_id
    is_analysis = sid in C.ANALYSIS_SCENARIO_IDS
    is_drafting = sid in C.MESSAGE_DRAFTING_SCENARIO_IDS
    is_no_rp    = sid in C.NO_RP_SCENARIO_IDS

    assistants = [m for m in (task.conversation_history or [])
                  if m.get("role") == "assistant"]

    # ------------------------------------------------------------------
    # If incomplete responses are allowed, enforce only the baseline.
    # ------------------------------------------------------------------
    if ALLOW_INCOMPLETE_RESPONSES:
        return any(len(m.get("content", "").strip()) >= _MIN_RAW_LEN
                   for m in assistants)

    # ── NO_RP / analysis ───────────────────────────────────────────────
    if is_analysis or is_no_rp:
        return any(len(m.get("content", "").strip()) >= _MIN_RAW_LEN
                   for m in assistants)

    # ── role-play / drafting ───────────────────────────────────────────
    parsed = task.parsed_responses or []
    if len(parsed) < len(assistants):          # parsed list too short
        return False

    required = (MANDATORY_SECTIONS_DRAFTING
                if is_drafting else
                MANDATORY_SECTIONS_ROLEPLAY)

    def turn_ok(pr: dict) -> bool:
        raw = (pr.get("raw") or "").strip()
        if len(raw) >= _MIN_RAW_LEN:
            return True                        # long raw content suffices
        return all(pr.get(k, "").strip() for k in required)

    if not all(turn_ok(parsed[i]) for i in range(len(assistants))):
        return False

    # Debrief is compulsory for role-play & drafting scenarios.
    return bool(task.debrief_response and task.debrief_response.strip())



def run_eq_bench3(
    model_name: str, # Logical name
    api_model_id: str, # API model ID
    # File Paths
    local_runs_file: str,
    local_elo_file: str,
    leaderboard_runs_file: str,
    leaderboard_elo_file: str,
    # Run Control
    num_threads: int = 4,
    run_id: Optional[str] = None,
    save_interval: int = 2,
    iterations: int = 1,
    # Feature Flags & Models
    run_elo: bool = True,
    run_rubric: bool = True,
    judge_model: Optional[str] = None, # Judge model ID string
    redo_judging: bool = False,
    truncate_for_rubric: bool = False,
) -> str:
    """
    Main function to run the EQBench3 benchmark.
    Orchestrates scenario simulation, debriefing, optional rubric scoring,
    and optional ELO analysis across iterations. Uses asynchronous saving.
    Handles standard, message drafting, and analysis task types.
    Uses logical model_name for tracking and api_model_id for API calls.
    Loads leaderboard data for context but writes ONLY to local files.
    """
    # --- Argument Validation ---
    if run_elo and not judge_model:
         raise ValueError("A --judge-model must be specified when running ELO analysis (--no-elo not set).")
    if run_rubric and not judge_model:
         raise ValueError("A --judge-model must be specified when running Rubric scoring (--no-rubric not set).")

    # --- Load Leaderboard Data (Read-Only) ---
    logging.info(f"Loading leaderboard runs data from: {leaderboard_runs_file}")
    leaderboard_runs = load_json_file(leaderboard_runs_file)
    logging.info(f"Loading leaderboard ELO data from: {leaderboard_elo_file}")
    leaderboard_elo = load_json_file(leaderboard_elo_file) # Loaded here, passed to ELO function

    # --- Load Local Data (Read/Write) ---
    logging.info(f"Loading local runs data from: {local_runs_file}")
    local_runs = load_json_file(local_runs_file)

    # --- Duplicate Model Name Check (Across Leaderboard & Local, only if ELO enabled) ---
    if run_elo:
        logging.info(f"Checking for duplicate model name '{model_name}' in run files (ELO enabled)...")
        # Check Leaderboard Runs
        for existing_run_key, run_data in leaderboard_runs.items():
            if isinstance(run_data, dict):
                existing_model = run_data.get("model_name", run_data.get("test_model"))
                if existing_model == model_name:
                    raise ValueError(
                        f"\nERROR: Logical model name '{model_name}' already exists in the LEADERBOARD runs file ('{leaderboard_runs_file}') under run key '{existing_run_key}'.\n"
                        f"       Unique model names are required when ELO is enabled to ensure comparisons are correctly attributed.\n"
                        f"       Please choose a different --model-name."
                    )

        # Check Local Runs (excluding the run being resumed, if applicable)
        for existing_run_key, run_data in local_runs.items():
            if isinstance(run_data, dict):
                existing_model = run_data.get("model_name", run_data.get("test_model"))
                if existing_model == model_name:
                    # Check if this is the exact run we are trying to resume
                    is_resuming_this_run = False
                    if run_id: # run_id is the prefix passed via CLI
                        # Construct potential run key for comparison
                        sanitized_model_for_check = re.sub(r'[^a-zA-Z0-9_.-]+', '_', model_name)
                        potential_resume_key = f"{run_id}_{sanitized_model_for_check}"
                        if existing_run_key == potential_resume_key:
                            is_resuming_this_run = True

                    if not is_resuming_this_run:
                        raise ValueError(
                            f"\nERROR: Logical model name '{model_name}' already exists in the LOCAL runs file ('{local_runs_file}') under run key '{existing_run_key}'.\n"
                            f"       Unique model names (that don't collide with other runs) are required when ELO is enabled.\n"
                            f"       This prevents ELO analysis from incorrectly reusing comparisons from a different run/version of the model.\n"
                            f"       Please choose a different --model-name or resume the existing run using --run-id {existing_run_key.split('_')[0]}"
                        )
        logging.info(f"Model name '{model_name}' is unique across run files.")
    else:
        logging.info("Skipping duplicate model name check as ELO is disabled.")


    # --- Run Key Setup ---
    def sanitize_model_name(name: str) -> str:
        # Sanitize the logical model name for the run key
        return re.sub(r'[^a-zA-Z0-9_.-]+', '_', name)

    sanitized_model = sanitize_model_name(model_name) # Use logical name here
    base_id = run_id if run_id else str(uuid.uuid4().hex[:8])
    run_key = f"{base_id}_{sanitized_model}"

    # --- Init or resume run (in Local Runs) ---
    if run_key not in local_runs:
        init_dict = {
            "run_key": run_key,
            "model_name": model_name, # Store logical name
            "api_model_id": api_model_id, # Store API ID
            "test_model": model_name, # Store logical name in legacy field
            "judge_model": judge_model if (run_elo or run_rubric) else "N/A",
            "start_time": datetime.now(timezone.utc).isoformat(),
            "status": "initializing",
            # Store paths used for reference (using constants)
            "scenario_prompts_file": C.STANDARD_SCENARIO_PROMPTS_FILE,
            "scenario_master_prompt_file": C.STANDARD_MASTER_PROMPT_FILE,
            "message_drafting_master_prompt_file": C.MESSAGE_DRAFTING_MASTER_PROMPT_FILE,
            "analysis_master_prompt_file": C.ANALYSIS_MASTER_PROMPT_FILE,
            "debrief_prompt_file": C.STANDARD_DEBRIEF_PROMPT_FILE,
            "rubric_criteria_file_standard": C.STANDARD_RUBRIC_CRITERIA_FILE if run_rubric else "N/A",
            "rubric_prompt_file_standard": C.STANDARD_RUBRIC_PROMPT_FILE if run_rubric else "N/A",
            "rubric_criteria_file_analysis": C.ANALYSIS_RUBRIC_CRITERIA_FILE if run_rubric else "N/A",
            "rubric_prompt_file_analysis": C.ANALYSIS_RUBRIC_PROMPT_FILE if run_rubric else "N/A",
            "truncate_for_rubric": truncate_for_rubric,
            "iterations_requested": iterations,
            "scenario_tasks": {},
            "results": {}
        }
        # Update ONLY the local runs file
        update_run_data(local_runs_file, run_key, init_dict)
        logging.info(f"Created new run in local file: {run_key} for model '{model_name}' (API ID: '{api_model_id}')")
        local_runs = load_json_file(local_runs_file)  # Reload local runs after update
    else:
        logging.info(f"Resuming run: {run_key} for model '{model_name}' (API ID: '{api_model_id}') from local file {local_runs_file}")
        # Minimal update logic for resuming, mainly status and start time if needed
        update_payload = {}
        current_run_data = local_runs[run_key] # Read from local_runs
        # Check if the model identifiers match the resumed run
        existing_model_name = current_run_data.get("model_name", current_run_data.get("test_model"))
        existing_api_id = current_run_data.get("api_model_id", current_run_data.get("test_model")) # Fallback needed?
        if existing_model_name != model_name:
             logging.warning(f"Resuming run {run_key} but logical model name mismatch! Run has '{existing_model_name}', requested '{model_name}'. Continuing with run's name.")
             # Use the name already associated with the run_key
             model_name = existing_model_name
        if existing_api_id != api_model_id:
             logging.warning(f"Resuming run {run_key} but API model ID mismatch! Run has '{existing_api_id}', requested '{api_model_id}'. Updating API ID in run data.")
             # Update the API ID in the run data if it changed
             update_payload["api_model_id"] = api_model_id

        if "start_time" not in current_run_data:
            update_payload["start_time"] = datetime.now(timezone.utc).isoformat()
        if current_run_data.get("status") not in ["running", "initializing", "completed_with_errors", "error"]: # Allow resuming from intermediate/error states
            current_status = current_run_data.get("status")
            logging.info(f"Run {run_key} status is '{current_status}'. Resetting status to 'running'.")
            update_payload["status"] = "running"
        # Add missing config info if resuming an older run format
        if "model_name" not in current_run_data: update_payload["model_name"] = model_name
        if "api_model_id" not in current_run_data: update_payload["api_model_id"] = api_model_id
        if "test_model" not in current_run_data: update_payload["test_model"] = model_name # Backfill legacy field
        if "iterations_requested" not in current_run_data: update_payload["iterations_requested"] = iterations
        # Add missing file paths if needed (less critical now, but good for consistency)
        if "scenario_master_prompt_file" not in current_run_data: update_payload["scenario_master_prompt_file"] = C.STANDARD_MASTER_PROMPT_FILE
        if "message_drafting_master_prompt_file" not in current_run_data: update_payload["message_drafting_master_prompt_file"] = C.MESSAGE_DRAFTING_MASTER_PROMPT_FILE
        if "analysis_master_prompt_file" not in current_run_data: update_payload["analysis_master_prompt_file"] = C.ANALYSIS_MASTER_PROMPT_FILE
        if run_rubric:
            if "rubric_criteria_file_standard" not in current_run_data: update_payload["rubric_criteria_file_standard"] = C.STANDARD_RUBRIC_CRITERIA_FILE
            if "rubric_prompt_file_standard" not in current_run_data: update_payload["rubric_prompt_file_standard"] = C.STANDARD_RUBRIC_PROMPT_FILE
            if "rubric_criteria_file_analysis" not in current_run_data: update_payload["rubric_criteria_file_analysis"] = C.ANALYSIS_RUBRIC_CRITERIA_FILE
            if "rubric_prompt_file_analysis" not in current_run_data: update_payload["rubric_prompt_file_analysis"] = C.ANALYSIS_RUBRIC_PROMPT_FILE
            if "truncate_for_rubric" not in current_run_data: update_payload["truncate_for_rubric"] = truncate_for_rubric

        if update_payload:
            # Update ONLY the local runs file
            update_run_data(local_runs_file, run_key, update_payload)
            local_runs = load_json_file(local_runs_file) # Reload local runs after update

    # --- Merge Run Data for Processing ---
    # Local runs override leaderboard runs on key collision
    merged_runs = {**leaderboard_runs, **local_runs}
    logging.info(f"Merged run data: {len(leaderboard_runs)} leaderboard runs, {len(local_runs)} local runs -> {len(merged_runs)} total runs for context.")


    # --- Redo Judging Logic (Reset tasks in the LOCAL file) ---
    if redo_judging and run_rubric:
        logging.info(f"Processing --redo-judging flag: resetting tasks in LOCAL file {local_runs_file}...")
        # Load local data directly for modification
        current_local_runs_data = load_json_file(local_runs_file)
        my_run_data = current_local_runs_data.get(run_key, {})
        scenario_tasks_data = my_run_data.get("scenario_tasks", {})

        updated_scenario_tasks = {}
        tasks_reset_count = 0

        for iter_str, scenario_dict in scenario_tasks_data.items():
            if not isinstance(scenario_dict, dict):
                updated_scenario_tasks[iter_str] = scenario_dict
                continue

            updated_scen_dict = {}
            for sid, task_info in scenario_dict.items():
                if isinstance(task_info, dict) and task_info.get("status") == "rubric_scored":
                    new_task_info = task_info.copy()
                    is_analysis = sid in C.ANALYSIS_SCENARIO_IDS
                    reset_status = "scenario_completed" if is_analysis else "completed"
                    new_task_info["status"] = reset_status
                    # Clear old rubric data
                    new_task_info.pop("rubric_scores", None)
                    new_task_info.pop("raw_rubric_judge_text", None)
                    new_task_info.pop("rubric_run_error", None)
                    tasks_reset_count += 1
                    updated_scen_dict[sid] = new_task_info
                    logging.debug(f"Resetting task {sid} (Iter {iter_str}) to '{reset_status}' for rubric re-judging.")
                else:
                    updated_scen_dict[sid] = task_info # Keep others

            updated_scenario_tasks[iter_str] = updated_scen_dict

        if tasks_reset_count > 0:
            # Update ONLY the local runs file
            update_success = update_run_data(local_runs_file, run_key, {"scenario_tasks": updated_scenario_tasks})
            if update_success:
                logging.info(f"[redo-judging] Reset {tasks_reset_count} task(s) in {local_runs_file}. The rubric scoring step will be re-run.")
            else:
                 logging.error(f"[redo-judging] Failed to save the reset task data to the local runs file: {local_runs_file}.")
            local_runs = load_json_file(local_runs_file) # Reload local runs after potential modification
            merged_runs = {**leaderboard_runs, **local_runs} # Re-merge after modification
        else:
            logging.info("[redo-judging] No tasks in 'rubric_scored' status were found in the local run to reset.")
    elif redo_judging and not run_rubric:
         logging.warning("--redo-judging flag ignored because --no-rubric is set.")

    # --- Load Prompts and Templates (Remains the same) ---
    try:
        scenarios = parse_scenario_prompts(C.STANDARD_SCENARIO_PROMPTS_FILE)
        if not scenarios:
            logging.error(f"No scenarios parsed from {C.STANDARD_SCENARIO_PROMPTS_FILE}. Aborting.")
            update_run_data(local_runs_file, run_key, {"status": "error", "error": "No scenarios parsed"}) # Write error to local
            return run_key
    except Exception as e:
        logging.error(f"Failed to load or parse scenario prompts: {e}", exc_info=True)
        update_run_data(local_runs_file, run_key, {"status": "error", "error": f"Failed to load scenarios: {e}"}) # Write error to local
        return run_key

    # Load Master Prompt Templates (Remains the same)
    try:
        standard_master_template = Path(C.STANDARD_MASTER_PROMPT_FILE).read_text(encoding='utf-8')
        if not standard_master_template.strip(): raise ValueError("Standard master prompt template file is empty.")
        logging.info(f"Loaded standard master prompt template from {C.STANDARD_MASTER_PROMPT_FILE}")

        drafting_master_template = Path(C.MESSAGE_DRAFTING_MASTER_PROMPT_FILE).read_text(encoding='utf-8')
        if not drafting_master_template.strip(): raise ValueError("Drafting master prompt template file is empty.")
        logging.info(f"Loaded drafting master prompt template from {C.MESSAGE_DRAFTING_MASTER_PROMPT_FILE}")

        analysis_master_template = Path(C.ANALYSIS_MASTER_PROMPT_FILE).read_text(encoding='utf-8')
        if not analysis_master_template.strip(): raise ValueError("Analysis master prompt template file is empty.")
        logging.info(f"Loaded analysis master prompt template from {C.ANALYSIS_MASTER_PROMPT_FILE}")

    except Exception as e:
        logging.error(f"Failed to load one or more master prompt templates: {e}", exc_info=True)
        update_run_data(local_runs_file, run_key, {"status": "error", "error": f"Failed to load master prompt template: {e}"}) # Write error to local
        return run_key

    # Load Debrief Prompt (only for standard/drafting) (Remains the same)
    try:
        standard_debrief_prompt = Path(C.STANDARD_DEBRIEF_PROMPT_FILE).read_text(encoding='utf-8')
        if not standard_debrief_prompt.strip(): raise ValueError("Debrief prompt file is empty.")
        logging.info(f"Loaded standard debrief prompt from {C.STANDARD_DEBRIEF_PROMPT_FILE}")
    except Exception as e:
        logging.error(f"Failed to load debrief prompt from {C.STANDARD_DEBRIEF_PROMPT_FILE}: {e}", exc_info=True)
        update_run_data(local_runs_file, run_key, {"status": "error", "error": f"Failed to load debrief prompt: {e}"}) # Write error to local
        return run_key


    # --- Load Rubric Scoring Files (if enabled) (Remains the same) ---
    standard_rubric_criteria = []
    standard_rubric_prompt_template = None
    standard_rubric_output_format_str = "{}"
    analysis_rubric_criteria = []
    analysis_rubric_prompt_template = None
    analysis_rubric_output_format_str = "{}"

    if run_rubric:
        # Load Standard Rubric Files
        try:
            with open(C.STANDARD_RUBRIC_CRITERIA_FILE, 'r', encoding='utf-8') as f:
                standard_rubric_criteria = [line.strip() for line in f if line.strip() and not line.strip().startswith('#')]
            if not standard_rubric_criteria: raise ValueError("Standard rubric criteria file is empty.")
            logging.info(f"Loaded {len(standard_rubric_criteria)} standard rubric criteria from {C.STANDARD_RUBRIC_CRITERIA_FILE}")

            output_format_dict_std = {"chain_of_thought_reasoning": "detailed chain of thought reasoning about the coming scoring decisions"}
            for criterion in standard_rubric_criteria: output_format_dict_std[criterion] = 0
            standard_rubric_output_format_str = json.dumps(output_format_dict_std, indent=2).replace(': 0', ': 0-20')

            standard_rubric_prompt_template = Path(C.STANDARD_RUBRIC_PROMPT_FILE).read_text(encoding='utf-8')
            if not standard_rubric_prompt_template or "{transcript}" not in standard_rubric_prompt_template or "{debrief}" not in standard_rubric_prompt_template or "{output_format}" not in standard_rubric_prompt_template:
                 raise ValueError("Standard rubric prompt template missing required placeholders ({transcript}, {debrief}, {output_format}).")
            logging.info(f"Loaded standard rubric prompt template from {C.STANDARD_RUBRIC_PROMPT_FILE}")

        except Exception as e:
            logging.error(f"Failed to load standard rubric files: {e}", exc_info=True)
            update_run_data(local_runs_file, run_key, {"status": "error", "error": f"Failed to load standard rubric files: {e}"}) # Write error to local
            return run_key

        # Load Analysis Rubric Files
        try:
            with open(C.ANALYSIS_RUBRIC_CRITERIA_FILE, 'r', encoding='utf-8') as f:
                analysis_rubric_criteria = [line.strip() for line in f if line.strip() and not line.strip().startswith('#')]
            if not analysis_rubric_criteria: raise ValueError("Analysis rubric criteria file is empty.")
            logging.info(f"Loaded {len(analysis_rubric_criteria)} analysis rubric criteria from {C.ANALYSIS_RUBRIC_CRITERIA_FILE}")

            output_format_dict_anl = {"chain_of_thought_reasoning": "detailed chain of thought reasoning about the coming scoring decisions"}
            for criterion in analysis_rubric_criteria: output_format_dict_anl[criterion] = 0
            analysis_rubric_output_format_str = json.dumps(output_format_dict_anl, indent=2).replace(': 0', ': 0-20') # Assuming 0-20 scale

            analysis_rubric_prompt_template = Path(C.ANALYSIS_RUBRIC_PROMPT_FILE).read_text(encoding='utf-8')
            # Analysis prompt should NOT have {debrief} placeholder
            if not analysis_rubric_prompt_template or "{transcript}" not in analysis_rubric_prompt_template or "{output_format}" not in analysis_rubric_prompt_template:
                 raise ValueError("Analysis rubric prompt template missing required placeholders ({transcript}, {output_format}).")
            if "{debrief}" in analysis_rubric_prompt_template:
                 logging.warning(f"Analysis rubric prompt template ({C.ANALYSIS_RUBRIC_PROMPT_FILE}) contains a '{{debrief}}' placeholder, which is not used for analysis tasks.")
            logging.info(f"Loaded analysis rubric prompt template from {C.ANALYSIS_RUBRIC_PROMPT_FILE}")

        except Exception as e:
            logging.error(f"Failed to load analysis rubric files: {e}", exc_info=True)
            update_run_data(local_runs_file, run_key, {"status": "error", "error": f"Failed to load analysis rubric files: {e}"}) # Write error to local
            return run_key

        logging.info(f"Rubric scoring enabled. Truncation for rubric: {truncate_for_rubric}")
    else:
        logging.info("Rubric scoring is disabled.")


    # --- Build API clients (Remains the same) ---
    api_clients = {"test": APIClient(model_type="test")}
    if run_elo or run_rubric:
        api_clients["judge"] = APIClient(model_type="judge")
        judge_usage = []
        if run_rubric: judge_usage.append("Rubric")
        if run_elo: judge_usage.append("ELO")
        logging.info(f"Judge model ({'/'.join(judge_usage)}): {judge_model}")


    # --- Prepare Task Objects (Load or Create from Merged Data) ---
    # Use merged_runs to find existing task data, allowing resumption of leaderboard tasks locally
    run_data_for_tasks = merged_runs.get(run_key, {})
    existing_tasks_data = run_data_for_tasks.get("scenario_tasks", {})
    tasks_to_process: List[ScenarioTask] = []

    total_tasks_expected = len(scenarios) * iterations
    logging.info(f"Preparing {total_tasks_expected} total tasks ({len(scenarios)} scenarios x {iterations} iterations)...")

    for i in range(1, iterations + 1):
        i_str = str(i)
        for scenario_id, prompts_list in scenarios.items():
            # Determine task type and select appropriate templates/prompts (Remains the same)
            is_analysis = scenario_id in C.ANALYSIS_SCENARIO_IDS
            is_drafting = scenario_id in C.MESSAGE_DRAFTING_SCENARIO_IDS

            chosen_master_template = None
            chosen_debrief_prompt = None
            if is_analysis:
                chosen_master_template = analysis_master_template
                chosen_debrief_prompt = None # Analysis tasks have no debrief
            elif is_drafting:
                chosen_master_template = drafting_master_template
                chosen_debrief_prompt = standard_debrief_prompt
            else: # Standard role-play
                chosen_master_template = standard_master_template
                chosen_debrief_prompt = standard_debrief_prompt

            task_obj = None
            if (i_str in existing_tasks_data and
                    scenario_id in existing_tasks_data[i_str]):
                task_data = existing_tasks_data[i_str][scenario_id]
                # Check if loaded task data matches the current run's logical model name
                # 'test_model' key holds the logical name in task data
                task_model_name = task_data.get("test_model")
                if isinstance(task_data, dict) and task_model_name == model_name:
                    try:
                        task_obj = ScenarioTask.from_dict(task_data)
                        # Update templates/prompts in case they changed or were missing
                        task_obj.master_prompt_template = chosen_master_template
                        task_obj.debrief_prompt = chosen_debrief_prompt # Update debrief prompt (or set to None for analysis)

                        if task_obj.iteration_index != i:
                            logging.warning(f"Mismatch iteration index in loaded task data for {scenario_id} (expected {i}, got {task_obj.iteration_index}). Resetting.")
                            task_obj.iteration_index = i
                        logging.debug(f"Resuming task: Scenario {scenario_id}, Iteration {i}, Status: {task_obj.status}")
                    except Exception as e:
                        logging.error(f"Failed to load task from dict for scenario={scenario_id}, iter={i}: {e}. Creating new task.", exc_info=True)
                        task_obj = None
                else:
                    # Model name mismatch or invalid data, create new task
                    if isinstance(task_data, dict) and task_model_name != model_name:
                         logging.warning(f"Task data found for scenario={scenario_id}, iter={i} belongs to a different model ('{task_model_name}' vs '{model_name}'). Creating new task.")
                    else:
                         logging.warning(f"Invalid or mismatched task data found for scenario={scenario_id}, iter={i}. Creating new task.")
                    task_obj = None

            if task_obj is None:
                task_obj = ScenarioTask(
                    scenario_id=scenario_id,
                    prompts=prompts_list,
                    debrief_prompt=chosen_debrief_prompt, # Pass None for analysis
                    iteration_index=i,
                    test_model=model_name, # Pass logical name to task constructor
                    master_prompt_template=chosen_master_template,
                )
                logging.debug(f"Creating new task: Scenario {scenario_id}, Iteration {i} (Analysis: {is_analysis})")

            tasks_to_process.append(task_obj)

    logging.info(f"Prepared {len(tasks_to_process)} task objects across {iterations} iteration(s).")


    # --- Setup Asynchronous Saving (Targeting LOCAL file) ---
    save_queue = queue.Queue()
    save_thread = threading.Thread(
        target=_save_worker,
        args=(save_queue, local_runs_file), # Pass the LOCAL runs file path
        name="SaveWorkerThread",
        daemon=True
    )
    save_thread.start()
    logging.info("Save worker thread started.")

    # --- Execute Tasks (Remains largely the same, save worker handles target file) ---
    tasks_completed_this_run = 0 # Tracks tasks fully completed (incl. rubric if enabled)

    try:
        # 1. Run scenario steps
        tasks_needing_scenario = [t for t in tasks_to_process if t.status in ["initialized", "error"]]
        if tasks_needing_scenario:
            logging.info(f"Running scenario simulation for {len(tasks_needing_scenario)} tasks...")
            with ThreadPoolExecutor(max_workers=num_threads, thread_name_prefix="ScenarioRun") as executor:
                futures = {
                    # Pass api_model_id for API calls
                    executor.submit(t.run_scenario, api_clients, save_queue, run_key, api_model_id): t
                    for t in tasks_needing_scenario
                }
                future_list = list(futures.keys())
                for future in tqdm(as_completed(future_list), total=len(future_list), desc="Running Scenarios"):
                    task = futures[future]
                    try:
                        future.result() # Wait for completion, errors handled within method
                    except Exception as e:
                        logging.error(f"Unhandled executor error during scenario for task {task.scenario_id} (Iter {task.iteration_index}): {e}", exc_info=True)
                        # Check status before overwriting, run_scenario sets error status internally
                        if task.status not in ["error", "scenario_completed", "completed", "rubric_scored"]:
                             task.status = "error"; task.error = f"Unhandled Executor Error: {e}"; task._save_progress(save_queue, run_key)

        else:
            logging.info("No tasks require scenario simulation based on initial status.")


        # 2. Run debrief steps (Skip for Analysis tasks)
        tasks_needing_debrief = [t for t in tasks_to_process if t.status == "scenario_completed" and t.scenario_id not in C.ANALYSIS_SCENARIO_IDS]
        if tasks_needing_debrief:
            logging.info(f"Running debrief for {len(tasks_needing_debrief)} non-analysis tasks...")
            with ThreadPoolExecutor(max_workers=num_threads, thread_name_prefix="DebriefRun") as executor:
                futures = {
                    # Pass api_model_id for API calls
                    executor.submit(t.run_debrief, api_clients, save_queue, run_key, api_model_id): t
                    for t in tasks_needing_debrief
                }
                future_list = list(futures.keys())
                for future in tqdm(as_completed(future_list), total=len(future_list), desc="Running Debriefs"):
                    task = futures[future]
                    try:
                        future.result()
                    except Exception as e:
                        logging.error(f"Unhandled executor error during debrief for task {task.scenario_id} (Iter {task.iteration_index}): {e}", exc_info=True)
                        # Check status before overwriting
                        if task.status not in ["error", "completed", "rubric_scored"]:
                             task.status = "error"; task.error = f"Unhandled Executor Error (Debrief): {e}"; task._save_progress(save_queue, run_key)
        else:
            logging.info("No non-analysis tasks require debriefing based on current status.")


        # 3. Run Rubric Scoring steps (if enabled) - Handles different task types
        if run_rubric:
            # Standard/Drafting tasks need rubric if status is 'completed'
            # Analysis tasks need rubric if status is 'scenario_completed'
            tasks_needing_rubric = [
                t for t in tasks_to_process
                if (
                    (
                        t.scenario_id in C.ANALYSIS_SCENARIO_IDS and t.status == "scenario_completed"
                    ) or (
                        t.scenario_id not in C.ANALYSIS_SCENARIO_IDS and t.status == "completed"
                    )
                )
                and _task_has_all_expected_responses(t)          # <<< new guard
            ]

            if tasks_needing_rubric:
                logging.info(f"Running rubric scoring for {len(tasks_needing_rubric)} tasks using judge model '{judge_model}' (Truncation: {truncate_for_rubric})...")
                with ThreadPoolExecutor(max_workers=num_threads, thread_name_prefix="RubricRun") as executor:
                    futures = {}
                    for t in tasks_needing_rubric:
                        is_analysis = t.scenario_id in C.ANALYSIS_SCENARIO_IDS
                        # Select appropriate rubric template and format string
                        rubric_template = analysis_rubric_prompt_template if is_analysis else standard_rubric_prompt_template
                        rubric_format = analysis_rubric_output_format_str if is_analysis else standard_rubric_output_format_str

                        future = executor.submit(
                            _execute_rubric_scoring_task,
                            task=t,
                            api_clients=api_clients,
                            judge_model_id=judge_model,
                            rubric_prompt_template=rubric_template,
                            rubric_output_format_str=rubric_format,
                            save_queue=save_queue,
                            run_key=run_key,
                            truncate_for_rubric=truncate_for_rubric
                        )
                        futures[future] = t

                    future_list = list(futures.keys())
                    for future in tqdm(as_completed(future_list), total=len(future_list), desc="Running Rubric Scoring"):
                        task = futures[future]
                        try:
                            future.result() # Wait for thread completion. Errors handled inside helper.
                            # Log progress based on tasks reaching the final state
                            if task.status == "rubric_scored":
                                tasks_completed_this_run += 1
                                if save_interval > 0 and (tasks_completed_this_run % save_interval == 0):
                                    logging.info(f"Completed {tasks_completed_this_run} tasks (incl. rubric) in this run.")
                        except Exception as e:
                            # This catches errors *outside* the helper's try/except
                            logging.error(
                                f"Unhandled executor error during rubric scoring future processing for task {task.scenario_id} (Iter {task.iteration_index}): {e}",
                                exc_info=True
                            )
                            # Ensure task status reflects error if it wasn't already set
                            if task.status not in ["error", "rubric_scored"]:
                                 task.status = "error"
                                 task.error = f"Unhandled Executor Error (Rubric Future): {e}"
                                 task._save_progress(save_queue, run_key) # Attempt to save error state
            else:
                logging.info("No tasks require rubric scoring based on current status.")
        else:
            # If rubric is disabled, count tasks reaching 'completed' (standard/drafting)
            # or 'scenario_completed' (analysis) as done.
            tasks_reaching_final_state = sum(
                1 for t in tasks_to_process
                if (t.scenario_id in C.ANALYSIS_SCENARIO_IDS and t.status == "scenario_completed") or \
                   (t.scenario_id not in C.ANALYSIS_SCENARIO_IDS and t.status == "completed")
            )
            tasks_completed_this_run = tasks_reaching_final_state
            logging.info("Rubric scoring disabled. Tasks reaching their respective pre-rubric completed state are considered finished for this run.")


    finally:
        # Signal the save worker to exit and wait for it
        logging.info("All task processing submitted. Waiting for save queue to empty...")
        save_queue.put(None)
        save_queue.join()
        logging.info("Save queue finished processing.")
        save_thread.join(timeout=10)
        if save_thread.is_alive():
            logging.warning("Save worker thread did not terminate after queue processing and join timeout.")

    # --- Calculate Final Rubric Score (if enabled, using LOCAL data) ---
    if run_rubric:
        logging.info(f"Calculating final average rubric score from local file: {local_runs_file}...")
        # Load the latest local data for calculation
        final_local_run_data_for_rubric = load_json_file(local_runs_file).get(run_key, {})
        avg_rubric_score, rubric_err = calculate_final_rubric_score(final_local_run_data_for_rubric)

        # Update results in the LOCAL file
        current_local_run_data = load_json_file(local_runs_file).get(run_key, {})
        current_results = current_local_run_data.get("results", {})
        current_results["average_rubric_score"] = avg_rubric_score if avg_rubric_score is not None else "N/A"
        current_results["rubric_calculation_time"] = datetime.now(timezone.utc).isoformat()
        current_results["rubric_error"] = rubric_err
        update_run_data(local_runs_file, run_key, {"results": current_results})

        if rubric_err: logging.error(f"Rubric score calculation failed: {rubric_err}")
        elif avg_rubric_score is not None: logging.info(f"Final Average Rubric Score: {avg_rubric_score:.2f}")
        else: logging.warning("Rubric score calculation resulted in None, but no specific error message.")
    else:
         # Update results in the LOCAL file if skipped
         current_local_run_data = load_json_file(local_runs_file).get(run_key, {})
         current_results = current_local_run_data.get("results", {})
         if "average_rubric_score" not in current_results:
             current_results["average_rubric_score"] = "Skipped"; current_results["rubric_error"] = None
             update_run_data(local_runs_file, run_key, {"results": current_results})


    # --- Final ELO analysis (if enabled) ---
    final_elo_snapshot = {} # To store the solved ratings from the ELO run
    elo_error_msg = None # Initialize error message for ELO step

    # --- Reload local runs data AFTER saving is complete ---
    logging.info(f"Reloading local runs data from {local_runs_file} before ELO analysis...")
    local_runs = load_json_file(local_runs_file) # Reload the updated local runs
    merged_runs = {**leaderboard_runs, **local_runs} # Re-merge with the read-only leaderboard data
    logging.info(f"Refreshed merged run data: {len(leaderboard_runs)} leaderboard runs, {len(local_runs)} local runs -> {len(merged_runs)} total runs for ELO context.")

    if run_elo:
        logging.info("Starting ELO analysis using merged leaderboard/local data...")
        try:
            # Pass merged run data, leaderboard/local ELO paths
            # ELO function now loads prompts internally based on scenario type
            # Pass the logical model name as test_model
            # Capture the returned snapshot and error message
            final_elo_snapshot, elo_error_msg = run_elo_analysis_eqbench3(
                run_key=run_key,
                # ELO Files
                leaderboard_elo_file=leaderboard_elo_file, # Passed into run_eq_bench3
                local_elo_file=local_elo_file,             # Passed into run_eq_bench3
                # Run Data
                merged_runs_data=merged_runs, # Use the merged data prepared earlier
                # Models
                test_model=model_name, # Logical name passed into run_eq_bench3
                judge_model=judge_model,
                api_clients=api_clients,
                # Other params
                scenarios_data=scenarios,
                concurrency=num_threads,
                recompute_existing=True
            )

            # Extract scores for the current model from the *solved* snapshot returned
            elo_raw, elo_norm = "N/A", "N/A"
            current_model_elo_data = final_elo_snapshot.get(model_name) # Use model_name passed into run_eq_bench3

            if isinstance(current_model_elo_data, dict):
                 elo_raw = current_model_elo_data.get("elo", "N/A")
                 elo_norm = current_model_elo_data.get("elo_norm", "N/A")
            elif isinstance(current_model_elo_data, (int, float)): # Handle older format if necessary
                 elo_raw = current_model_elo_data
                 # Attempt to get norm from potentially updated local file as fallback
                 final_local_elo_data = load_json_file(local_elo_file)
                 if isinstance(final_local_elo_data.get(model_name), dict):
                     elo_norm = final_local_elo_data[model_name].get("elo_norm", "N/A")

            # Update results in the LOCAL run file
            current_local_run_data = load_json_file(local_runs_file).get(run_key, {})
            current_results = current_local_run_data.get("results", {})
            current_results.update({
                "elo_raw": elo_raw,
                "elo_normalized": elo_norm,
                "elo_calculation_time": datetime.now(timezone.utc).isoformat(),
                "elo_error": elo_error_msg # Store error message from ELO run
            })
            update_run_data(local_runs_file, run_key, {"results": current_results})

            if elo_error_msg is None:
                logging.info(f"ELO scores for {model_name} (from solved snapshot): Raw={elo_raw}, Normalized={elo_norm}")
                # NO leaderboard printing here
            else:
                logging.error(f"ELO calculation finished with message: {elo_error_msg}")

        except FileNotFoundError as e:
            logging.error(f"ELO analysis skipped: Required file not found: {e}")
            elo_error_msg = f"File not found: {e}"
            current_local_run_data = load_json_file(local_runs_file).get(run_key, {})
            current_results = current_local_run_data.get("results", {})
            current_results.update({"elo_error": elo_error_msg, "elo_raw": "Error", "elo_normalized": "Error"})
            update_run_data(local_runs_file, run_key, {"results": current_results})
        except Exception as e:
            logging.error(f"ELO analysis failed: {e}", exc_info=True)
            elo_error_msg = str(e)
            current_local_run_data = load_json_file(local_runs_file).get(run_key, {})
            current_results = current_local_run_data.get("results", {})
            current_results.update({"elo_error": elo_error_msg, "elo_raw": "Error", "elo_normalized": "Error"})
            update_run_data(local_runs_file, run_key, {"results": current_results})
    else:
        logging.info("Skipping ELO analysis as per --no-elo flag.")
        # Update results in the LOCAL file if skipped
        current_local_run_data = load_json_file(local_runs_file).get(run_key, {})
        current_results = current_local_run_data.get("results", {})
        if "elo_raw" not in current_results: current_results["elo_raw"] = "Skipped"
        if "elo_normalized" not in current_results: current_results["elo_normalized"] = "Skipped"
        current_results["elo_error"] = None
        update_run_data(local_runs_file, run_key, {"results": current_results})


    # --- Mark run as completed or completed_with_errors (in LOCAL file) ---
    final_status = "completed"
    # Load final data from LOCAL file
    final_local_run_data = load_json_file(local_runs_file).get(run_key, {})
    final_tasks_data = final_local_run_data.get("scenario_tasks", {})
    tasks_in_error_count = 0
    tasks_not_fully_completed_count = 0
    error_examples = []

    for iter_str, scenarios_in_iter in final_tasks_data.items():
        if isinstance(scenarios_in_iter, dict):
            for scenario_id, task_data in scenarios_in_iter.items():
                if isinstance(task_data, dict):
                    task_status = task_data.get("status")
                    is_analysis = scenario_id in C.ANALYSIS_SCENARIO_IDS
                    # Define expected final state based on task type and whether rubric is run
                    if run_rubric:
                        final_expected_status = "rubric_scored"
                    else:
                        final_expected_status = "scenario_completed" if is_analysis else "completed"

                    if task_status == "error":
                        tasks_in_error_count += 1
                        if len(error_examples) < 5: error_examples.append(f"Iter {iter_str}, Scenario {scenario_id}: {task_data.get('error', 'Unknown error')}")
                    elif task_status != final_expected_status:
                         tasks_not_fully_completed_count += 1
                         if len(error_examples) < 5: error_examples.append(f"Iter {iter_str}, Scenario {scenario_id}: Status '{task_status}' (expected '{final_expected_status}')")

    if tasks_in_error_count > 0 or tasks_not_fully_completed_count > 0:
        final_status = "completed_with_errors"
        warning_msg = f"Run {run_key} finished, but issues detected: "
        if tasks_in_error_count > 0: warning_msg += f"{tasks_in_error_count} task(s) ended in error. "
        if tasks_not_fully_completed_count > 0: warning_msg += f"{tasks_not_fully_completed_count} task(s) did not reach final expected status. "
        logging.warning(warning_msg)
        for err_ex in error_examples: logging.warning(f"  - Example Issue: {err_ex}")

    # Update status in LOCAL file
    update_run_data(local_runs_file, run_key, {
        "status": final_status,
        "end_time": datetime.now(timezone.utc).isoformat()
    })
    logging.info(f"Run {run_key} marked as {final_status} in {local_runs_file}.")

    return run_key

# File: ai/eqbench3/core/conversation.py

# core/conversation.py

import time
import logging
import re
import json # For parsing rubric scores
from typing import Dict, Any, List, Optional, Tuple
import queue
from ..utils.constants import (
    NO_RP_SCENARIO_IDS,
    MESSAGE_DRAFTING_SCENARIO_IDS,
    ANALYSIS_SCENARIO_IDS, # Added
    SECTION_CHAR_LIMITS,
    SECTION_CHAR_LIMITS_MESSAGE_DRAFT,
    RAW_RESPONSE_CHAR_LIMIT,
    DEBRIEF_CHAR_LIMIT,
    ANALYSIS_RESPONSE_CHAR_LIMIT
)
from ..utils.utils import robust_json_loads

class ScenarioTask:
    """
    Represents a single multi-turn scenario simulation task for eqbench3.
    Handles running the scenario prompts sequentially, an optional final debrief prompt
    (skipped for analysis tasks), and provides helpers for an optional final rubric
    scoring step. Includes iteration_index.
    Manages its own state but relies on external orchestrator (benchmark.py) for API calls.
    Stores the logical model name internally.
    """

    def __init__(
        self,
        scenario_id: str,
        prompts: List[str],
        debrief_prompt: Optional[str], # Can be None for analysis tasks
        iteration_index: int,
        test_model: str, # This is the logical model_name passed from benchmark.py
        master_prompt_template: str = None,
    ):
        self.scenario_id = scenario_id
        self.prompts = prompts
        self.debrief_prompt = debrief_prompt # Store None if analysis task
        self.iteration_index = iteration_index
        self.model_name = test_model # Store the logical name internally
        self.master_prompt_template = master_prompt_template

        # Load superprompt from .cache if available
        self.superprompt = self._load_superprompt()

        # Status Lifecycle:
        # Standard/Drafting: initialized -> running_scenario -> scenario_completed -> running_debrief -> completed -> [running_rubric_scoring -> rubric_scored]
        # Analysis:          initialized -> running_scenario -> scenario_completed -> [running_rubric_scoring -> rubric_scored]
        # Any step can transition to 'error'.
        self.status = "initialized"
        self.start_time = None
        self.end_time = None # Marks end of the *entire* task processing (incl. rubric)
        self.error = None # General error message if any step fails

        # Scenario Phase Data
        self.conversation_history: List[Dict[str, str]] = []
        self.parsed_responses: List[Dict[str, str]] = [] # Stores parsed sections or just {"raw": ...} for NO_RP/ANALYSIS
        self.scenario_run_error: Optional[str] = None

        # Debrief Phase Data (Only for non-analysis tasks)
        self.debrief_response: Optional[str] = None
        self.debrief_run_error: Optional[str] = None

        # Rubric Scoring Phase Data
        self.rubric_scores: Optional[Dict[str, float]] = None
        self.raw_rubric_judge_text: Optional[str] = None
        self.rubric_run_error: Optional[str] = None

    @staticmethod
    def _load_superprompt():
        """Load superprompt from .cache/superprompt.txt if it exists."""
        superprompt_path = ".cache/superprompt.txt"
        try:
            with open(superprompt_path, 'r', encoding='utf-8') as f:
                content = f.read().strip()
                if content:
                    logging.info(f"Loaded superprompt from {superprompt_path} ({len(content)} chars)")
                    return content + "\n\n"
                else:
                    logging.debug(f"Superprompt file {superprompt_path} exists but is empty")
                    return ""
        except FileNotFoundError:
            logging.debug(f"No superprompt found at {superprompt_path}")
            return ""
        except Exception as e:
            logging.warning(f"Error loading superprompt from {superprompt_path}: {e}")
            return ""

        # Status Lifecycle:
        # Standard/Drafting: initialized -> running_scenario -> scenario_completed -> running_debrief -> completed -> [running_rubric_scoring -> rubric_scored]
        # Analysis:          initialized -> running_scenario -> scenario_completed -> [running_rubric_scoring -> rubric_scored]
        # Any step can transition to 'error'.
        self.status = "initialized"
        self.start_time = None
        self.end_time = None # Marks end of the *entire* task processing (incl. rubric)
        self.error = None # General error message if any step fails

        # Scenario Phase Data
        self.conversation_history: List[Dict[str, str]] = []
        self.parsed_responses: List[Dict[str, str]] = [] # Stores parsed sections or just {"raw": ...} for NO_RP/ANALYSIS
        self.scenario_run_error: Optional[str] = None

        # Debrief Phase Data (Only for non-analysis tasks)
        self.debrief_response: Optional[str] = None
        self.debrief_run_error: Optional[str] = None

        # Rubric Scoring Phase Data
        self.rubric_scores: Optional[Dict[str, float]] = None
        self.raw_rubric_judge_text: Optional[str] = None
        self.rubric_run_error: Optional[str] = None


    def _save_progress(self, save_queue: Optional[queue.Queue], run_key: Optional[str]):
        """Helper to put the current task state onto the save queue."""
        if save_queue and run_key:
            try:
                task_data = self.to_dict()
                save_queue.put((run_key, self.iteration_index, self.scenario_id, task_data))
                logging.debug(f"Task {self.scenario_id} (Iter {self.iteration_index}) data queued for saving (Status: {self.status}).")
            except Exception as e:
                 logging.error(f"Failed to queue save progress for task {self.scenario_id} (Iter {self.iteration_index}): {e}", exc_info=True)
        elif not save_queue:
             logging.warning("Save queue is None, cannot save progress.")
        elif not run_key:
             logging.warning("Run key is None, cannot save progress.")


    def _parse_response(self, response: str) -> Dict[str, str]:
        """
        Extract the structured sections from a model reply during scenario turns.
        Only applicable for standard role-play and message drafting scenarios.
        """
        # Patterns for different scenario types that require parsing
        # core/elo.py  (or wherever you define _parse_response)
        draft_patterns = {
            "perspective_taking":   r"#\s*Perspective[- ]taking\s*\n([\s\S]*?)(?=#|\Z)",
            "draft_brainstorming":  r"#\s*Draft brainstorming\s*\n([\s\S]*?)(?=#|\Z)",
            "draft":                r"#\s*Draft\s*\n([\s\S]*?)(?=--|\Z)",
        }

        # Accept straight (') **or** curly (’ U+2019) apostrophes
        rp_patterns = {
            "thinking_feeling":
                r"#\s*I[’']m thinking & feeling\s*\n([\s\S]*?)(?=#|\Z)",
            "their_thinking_feeling":
                r"#\s*They[’']re thinking & feeling\s*\n([\s\S]*?)(?=#|\Z)",
            "response":
                r"#\s*My response\s*\n([\s\S]*?)(?=--|\Z)",
        }


        # Determine which patterns to use
        if self.scenario_id in MESSAGE_DRAFTING_SCENARIO_IDS:
            patterns = draft_patterns
        elif self.scenario_id not in NO_RP_SCENARIO_IDS and self.scenario_id not in ANALYSIS_SCENARIO_IDS:
            patterns = rp_patterns
        else:
            # Should not be called for NO_RP or ANALYSIS, but return raw if it is
            logging.warning(f"_parse_response called unexpectedly for scenario {self.scenario_id}. Returning raw.")
            return {"raw": response}

        parsed = {k: "" for k in patterns}
        parsed["raw"] = response # Always include raw response

        for key, pat in patterns.items():
            m = re.search(pat, response, flags=re.IGNORECASE | re.DOTALL) # Added DOTALL
            if m:
                parsed[key] = m.group(1).strip()

        return parsed

    @staticmethod
    def _parse_rubric_scores(judge_response_text: str) -> Optional[Dict[str, float]]:
        """
        Parses the JSON response from the rubric scoring judge.
        Returns a dictionary of scores or None if parsing fails.
        Looks for content between matching { and } characters.
        """
        try:
            # Find the first '{' and last '}' to extract JSON content
            #start_idx = judge_response_text.find('{')
            #end_idx = judge_response_text.rfind('}')

            #if start_idx == -1 or end_idx == -1 or start_idx > end_idx:
            #    logging.warning(f"No valid JSON object found. Raw text: {judge_response_text}...")
            #    return None

            #json_str = judge_response_text[start_idx:end_idx+1].strip()
            parsed_data = robust_json_loads(judge_response_text)

            if not isinstance(parsed_data, dict):
                logging.warning(f"Rubric score parsing failed: Parsed data is not a dictionary. Raw text: {judge_response_text}...")
                return None
            
            if not parsed_data:
                logging.warning(f"Rubric score parsing yielded no numeric scores. Raw text: {judge_response_text}\n\nParsed: {json.dumps(parsed_data)}")
                return None

            scores = {}
            for key, value in parsed_data.items():
                if isinstance(value, (int, float)):
                    scores[key] = float(value)
                elif key != "chain_of_thought_reasoning": # Allow reasoning string
                    # Try to extract number if string contains one (e.g., "15/20")
                    num_match = re.search(r'\b(\d+(\.\d+)?)\b', str(value))
                    if num_match:
                        try:
                            scores[key] = float(num_match.group(1))
                            logging.debug(f"Extracted numeric score {scores[key]} from string '{value}' for key '{key}'.")
                        except ValueError:
                             logging.debug(f"Rubric score for '{key}' is non-numeric ('{value}'). Skipping.")
                    else:
                        logging.debug(f"Rubric score for '{key}' is non-numeric ('{value}'). Skipping.")

            if not scores:
                logging.warning(f"Rubric score parsing yielded no numeric scores. Raw text: {judge_response_text}\n\nParsed: {json.dumps(parsed_data)}")
                return None

            return scores

        except json.JSONDecodeError as e:
            logging.warning(f"Rubric score JSON parsing failed: {e}. Raw text: {judge_response_text}...")
            return None
        except Exception as e:
            logging.error(f"Unexpected error during rubric score parsing: {e}", exc_info=True)
            return None


    def run_scenario(self, api_clients: Dict[str, Any], save_queue: Optional[queue.Queue], run_key: Optional[str], api_model_id: str):
        """
        Runs the multi-turn scenario simulation sequentially using the test model.
        Updates conversation_history and status. Puts progress updates on the save_queue.
        Handles standard, drafting, NO_RP, and analysis types.
        Uses the provided api_model_id for API calls.
        """
        is_analysis = self.scenario_id in ANALYSIS_SCENARIO_IDS
        # Determine valid skip statuses based on type
        valid_skip_statuses = ["scenario_completed", "running_rubric_scoring", "rubric_scored"]
        if not is_analysis:
            valid_skip_statuses.extend(["running_debrief", "completed"])

        if self.status in valid_skip_statuses:
            logging.debug(f"Scenario task {self.scenario_id} (Iter {self.iteration_index}) status is '{self.status}'. Skipping scenario run.")
            return
        if self.status == "error":
             logging.info(f"Retrying scenario run for task {self.scenario_id} (Iter {self.iteration_index}) which previously errored.")

        logging.info(f"Starting scenario run for task {self.scenario_id} (Iter {self.iteration_index}, Analysis: {is_analysis}) with model {self.model_name} (API ID: {api_model_id})")
        self.status = "running_scenario"
        self.scenario_run_error = None; self.error = None
        if not self.start_time: self.start_time = time.time()

        test_api = api_clients["test"]
        if self.conversation_history and self.scenario_run_error: # Check specific error flag
            logging.info(f"Clearing previous history for errored task {self.scenario_id} (Iter {self.iteration_index}) before retry.")
            self.conversation_history = []; self.parsed_responses = []

        current_messages = list(self.conversation_history)
        turn_index = -1 # Initialize turn index

        try:
            for turn_index, user_prompt in enumerate(self.prompts):
                expected_len_after_this_turn = (turn_index + 1) * 2
                if len(current_messages) >= expected_len_after_this_turn:
                    logging.debug(f"Skipping already completed turn {turn_index + 1} for scenario {self.scenario_id} (Iter {self.iteration_index})")
                    continue

                logging.debug(f"Running turn {turn_index + 1}/{len(self.prompts)} for scenario {self.scenario_id} (Iter {self.iteration_index})")
                no_rp_scenario = self.scenario_id in NO_RP_SCENARIO_IDS

                # Apply master prompt template if provided and applicable
                if self.master_prompt_template and not no_rp_scenario:
                    # Master template is already selected based on type in benchmark.py
                    formatted_prompt = self.master_prompt_template.format(scenario_prompt=user_prompt)
                else:
                    # Fallback for NO_RP or if template is missing (shouldn't happen for analysis/drafting now)
                    formatted_prompt = user_prompt # Maybe add word count hint here too?

                # Prepend superprompt at the very first layer of user instructions
                if self.superprompt:
                    formatted_prompt = self.superprompt + formatted_prompt

                # Add user prompt to history
                if not current_messages or current_messages[-1]["role"] == "assistant":
                    current_messages.append({"role": "user", "content": formatted_prompt})
                elif current_messages[-1]["role"] == "user" and current_messages[-1]["content"] != formatted_prompt:
                     logging.warning(f"Overwriting last user message at turn {turn_index+1} for {self.scenario_id} (Iter {self.iteration_index}).")
                     current_messages[-1] = {"role": "user", "content": formatted_prompt}
                # Handle case where resuming exactly after user prompt was added but before assistant response
                elif current_messages[-1]["role"] == "user" and current_messages[-1]["content"] == formatted_prompt:
                     pass # Correct state, proceed to generate assistant response
                else: # Should not happen
                     logging.error(f"Unexpected history state at turn {turn_index+1} for {self.scenario_id}. Last msg: {current_messages[-1]['role']}")
                     # Attempt to recover by adding user prompt if missing
                     if current_messages[-1]["role"] != "user":
                         current_messages.append({"role": "user", "content": formatted_prompt})


                assistant_response = test_api.generate(
                    model=api_model_id, # Use the API model ID here
                    messages=current_messages,
                    temperature=0.7, # Consider different temps per type?
                    max_tokens=12000, # Consider different lengths per type?
                    min_p=0.1
                    )

                # Store response: Parse for standard/drafting, store raw for NO_RP/Analysis
                parsed_entry = {}
                if not no_rp_scenario and not is_analysis:
                    parsed_entry = self._parse_response(assistant_response)
                else:
                    # Store raw response in a consistent structure
                    parsed_entry = {"raw": assistant_response}

                # Ensure parsed_responses list is updated correctly
                if len(self.parsed_responses) == turn_index:
                    self.parsed_responses.append(parsed_entry)
                elif len(self.parsed_responses) > turn_index:
                    self.parsed_responses[turn_index] = parsed_entry # Overwrite if resuming/rerunning
                else:
                    # This indicates a logic error, list should grow sequentially
                    logging.error(f"Inconsistent parsed_responses length for {self.scenario_id} (Iter {self.iteration_index}) at turn {turn_index+1}. List len: {len(self.parsed_responses)}")
                    # Attempt recovery: append anyway?
                    self.parsed_responses.append(parsed_entry)


                # Add assistant response to history
                if len(current_messages) == (turn_index * 2) + 1: # Should be after user prompt
                    current_messages.append({"role": "assistant", "content": assistant_response})
                elif current_messages[-1]["role"] == "user": # Also valid if resuming exactly here
                     current_messages.append({"role": "assistant", "content": assistant_response})
                else: # Should not happen
                     logging.error(f"Unexpected history state before adding assistant response at turn {turn_index+1} for {self.scenario_id}. Last msg: {current_messages[-1]['role']}")
                     # Attempt to recover
                     current_messages.append({"role": "assistant", "content": assistant_response})


                self.conversation_history = list(current_messages)
                self._save_progress(save_queue, run_key)

            # Scenario completed successfully
            self.status = "scenario_completed" # Correct status for all types after scenario run
            logging.info(f"Scenario run finished successfully for task {self.scenario_id} (Iter {self.iteration_index})")

        except Exception as e:
            current_turn_for_error = turn_index + 1 if turn_index >= 0 else 1 # Adjust error reporting turn
            error_msg = f"Scenario Run Error at turn {current_turn_for_error}: {str(e)}"
            logging.error(f"Error during scenario run for task {self.scenario_id} (Iter {self.iteration_index}) at turn {current_turn_for_error}: {e}", exc_info=True)
            self.status = "error"; self.error = error_msg; self.scenario_run_error = str(e)
        finally:
            self._save_progress(save_queue, run_key)


    def run_debrief(self, api_clients: Dict[str, Any], save_queue: Optional[queue.Queue], run_key: Optional[str], api_model_id: str):
        """
        Runs the debrief prompt using the test model after the scenario is completed.
        SKIPS this step for Analysis tasks.
        Updates debrief_response and status. Puts progress updates on the save_queue.
        Uses the provided api_model_id for API calls.
        """
        # --- Skip entirely for Analysis tasks ---
        if self.scenario_id in ANALYSIS_SCENARIO_IDS:
            logging.debug(f"Skipping debrief step for analysis task {self.scenario_id} (Iter {self.iteration_index}). Status remains '{self.status}'.")
            # No status change needed, stays 'scenario_completed' ready for rubric
            return

        # --- Logic for non-analysis tasks ---
        if self.status not in ["scenario_completed", "error"]:
             logging.debug(f"Debrief cannot run for task {self.scenario_id} (Iter {self.iteration_index}). Status is '{self.status}'. Skipping.")
             return
        # If errored during scenario, skip debrief unless it was specifically a debrief error we are retrying
        if self.status == "error" and not self.debrief_run_error:
             logging.warning(f"Task {self.scenario_id} (Iter {self.iteration_index}) errored before debrief. Skipping debrief.")
             # Don't save progress here, error state is already saved
             return
        if self.status == "error" and self.debrief_run_error:
             logging.info(f"Retrying debrief run for task {self.scenario_id} (Iter {self.iteration_index}) which previously errored during debrief.")

        # If debrief already done (e.g., resuming), move to 'completed'
        if self.debrief_response is not None and self.status == "scenario_completed":
             logging.info(f"Debrief response already exists for task {self.scenario_id} (Iter {self.iteration_index}). Marking as completed (ready for rubric).")
             self.status = "completed"
             self._save_progress(save_queue, run_key)
             return
        # If already past debrief stage
        if self.status in ["completed", "running_rubric_scoring", "rubric_scored"]:
             logging.debug(f"Debrief already completed or passed for task {self.scenario_id} (Iter {self.iteration_index}). Status: '{self.status}'. Skipping.")
             return

        logging.info(f"Starting debrief run for task {self.scenario_id} (Iter {self.iteration_index})")
        self.status = "running_debrief"
        self.debrief_run_error = None; self.error = None # Clear previous errors for retry

        test_api = api_clients["test"]
        if not self.conversation_history:
             logging.error(f"Cannot run debrief for task {self.scenario_id} (Iter {self.iteration_index}): Conversation history is empty.")
             self.status = "error"; self.error = "Debrief Run Error: Conversation history is empty."; self.debrief_run_error = "Conversation history is empty."
             self._save_progress(save_queue, run_key)
             return
        if not self.debrief_prompt: # Should not happen for non-analysis tasks if loaded correctly
             logging.error(f"Cannot run debrief for task {self.scenario_id} (Iter {self.iteration_index}): Debrief prompt is missing.")
             self.status = "error"; self.error = "Debrief Run Error: Debrief prompt missing."; self.debrief_run_error = "Debrief prompt missing."
             self._save_progress(save_queue, run_key)
             return


        debrief_messages = list(self.conversation_history)
        debrief_messages.append({"role": "user", "content": self.debrief_prompt})

        try:
            response = test_api.generate(
                model=api_model_id, # Use the API model ID here
                messages=debrief_messages,
                temperature=0.5,
                max_tokens=12000,
                min_p=None
            )
            self.debrief_response = response.strip()
            self.status = "completed" # Final status before optional rubric step
            logging.info(f"Debrief run finished successfully for task {self.scenario_id} (Iter {self.iteration_index})")
        except Exception as e:
            error_msg = f"Debrief Run Error: {str(e)}"
            logging.error(f"Error during debrief run for task {self.scenario_id} (Iter {self.iteration_index}): {e}", exc_info=True)
            self.status = "error"; self.error = error_msg; self.debrief_run_error = str(e)
        finally:
            self._save_progress(save_queue, run_key)


    # --- Helper Methods for Truncation (Static) ---
    @staticmethod
    def _truncate_text(text: str, limit: int) -> str:
        """Truncates text to a character limit."""
        if not text or len(text) <= limit:
            return text
        return text[:limit] + "... [truncated]"

    @staticmethod
    def _truncate_parsed_sections(parsed: Dict[str, str], limits: Dict[str, int]) -> Dict[str, str]:
        """Truncate each section of parsed response according to character limits."""
        truncated = {}
        # Ensure all expected keys from limits are present, even if empty or not in parsed
        for section_name in limits:
            content = parsed.get(section_name) # Use .get for safety
            if content:
                char_limit = limits[section_name]
                truncated[section_name] = ScenarioTask._truncate_text(content, char_limit)
            else:
                truncated[section_name] = "" # Keep empty content as empty string
        return truncated


    @staticmethod
    def _format_parsed_response(sections: Dict[str, str]) -> str:
        """Render parsed sections back into a readable block."""
        formatted = []
        # Check for role-play keys first
        if "thinking_feeling" in sections:
            formatted.append("# I'm thinking & feeling")
            formatted.append(sections.get("thinking_feeling", ""))
            formatted.append("\n# They're thinking & feeling")
            formatted.append(sections.get("their_thinking_feeling", ""))
            formatted.append("\n# My response")
            formatted.append(sections.get("response", ""))
        # Check for message-drafting keys
        elif "perspective_taking" in sections:
            formatted.append("# Perspective-taking")
            formatted.append(sections.get("perspective_taking", ""))
            formatted.append("\n# Draft brainstorming")
            formatted.append(sections.get("draft_brainstorming", ""))
            formatted.append("\n# Draft")
            formatted.append(sections.get("draft", ""))
        else: # Fallback if keys don't match expected patterns
             logging.warning("Could not format parsed response: Unknown section keys.")
             # Attempt to join non-empty values from known section types
             known_keys = list(SECTION_CHAR_LIMITS.keys()) + list(SECTION_CHAR_LIMITS_MESSAGE_DRAFT.keys())
             content_parts = [v for k, v in sections.items() if k in known_keys and v]
             return "\n\n".join(content_parts)

        # Add the standard separator if sections were found
        if formatted:
            formatted.append("\n--")
        return "\n".join(formatted)


    # --- NEW Rubric Helper Methods ---

    def prepare_rubric_prompt_text(
        self,
        rubric_prompt_template: str, # Specific template for this task type
        rubric_output_format_str: str, # Specific format for this task type
        truncate_for_rubric: bool # Added flag
    ) -> Optional[str]:
        """
        Formats the rubric scoring prompt using the task's transcript and debrief (if applicable).
        Optionally truncates assistant responses in the transcript before formatting.
        Handles standard, drafting, and analysis task types.
        Returns the formatted prompt string, or None if essential data is missing.
        """
        is_analysis = self.scenario_id in ANALYSIS_SCENARIO_IDS

        if not self.conversation_history:
            logging.error(f"Cannot prepare rubric prompt for task {self.scenario_id} (Iter {self.iteration_index}): Conversation history is empty.")
            self.status = "error"; self.error = "Rubric Prep Error: History missing."; self.rubric_run_error = "History missing."
            return None
        # Debrief is required ONLY for non-analysis tasks
        if not is_analysis and self.debrief_response is None:
            logging.error(f"Cannot prepare rubric prompt for task {self.scenario_id} (Iter {self.iteration_index}): Debrief response is missing for non-analysis task.")
            self.status = "error"; self.error = "Rubric Prep Error: Debrief missing."; self.rubric_run_error = "Debrief missing."
            return None

        # Build the transcript string
        transcript_parts = []
        assistant_response_index = 0
        for msg_idx, msg in enumerate(self.conversation_history):
            role_label = "User" if msg.get("role") == "user" else "Assistant"
            raw_content = msg.get("content", "")
            processed_content = ""

            if role_label == "User":
                processed_content = raw_content # User content is used as is
            else: # Assistant message
                is_no_rp = self.scenario_id in NO_RP_SCENARIO_IDS
                is_drafting = self.scenario_id in MESSAGE_DRAFTING_SCENARIO_IDS

                # Determine how to process/truncate based on type and flag
                if truncate_for_rubric:
                    if is_no_rp:
                        # Truncate raw content for NO_RP and ANALYSIS scenarios
                        processed_content = ScenarioTask._truncate_text(raw_content, RAW_RESPONSE_CHAR_LIMIT)
                    elif is_analysis:
                        # Truncate raw content for NO_RP and ANALYSIS scenarios
                        processed_content = ScenarioTask._truncate_text(raw_content, ANALYSIS_RESPONSE_CHAR_LIMIT)
                    else:
                        # Truncate structured content for standard/drafting scenarios
                        try:
                            if assistant_response_index >= len(self.parsed_responses):
                                raise IndexError(f"Assistant response index {assistant_response_index} out of bounds for parsed_responses (len {len(self.parsed_responses)})")
                            parsed = self.parsed_responses[assistant_response_index]
                            # Use .get on parsed dict for safety
                            if not isinstance(parsed, dict):
                                 logging.warning(f"Parsed response at index {assistant_response_index} is not a dict for task {self.scenario_id}. Using raw truncated.")
                                 processed_content = ScenarioTask._truncate_text(raw_content, RAW_RESPONSE_CHAR_LIMIT)
                            else:
                                limits = SECTION_CHAR_LIMITS_MESSAGE_DRAFT if is_drafting else SECTION_CHAR_LIMITS
                                truncated_sections = ScenarioTask._truncate_parsed_sections(parsed, limits)
                                processed_content = ScenarioTask._format_parsed_response(truncated_sections)
                        except IndexError as e:
                            logging.error(f"{e} accessing parsed_responses for task {self.scenario_id} (Iter {self.iteration_index}). History len: {len(self.conversation_history)}, Parsed len: {len(self.parsed_responses)}. Using raw truncated content.")
                            processed_content = ScenarioTask._truncate_text(raw_content, RAW_RESPONSE_CHAR_LIMIT) # Fallback
                        except Exception as e:
                            logging.error(f"Error processing/truncating parsed response {assistant_response_index} for task {self.scenario_id} (Iter {self.iteration_index}): {e}. Using raw truncated content.", exc_info=True)
                            processed_content = ScenarioTask._truncate_text(raw_content, RAW_RESPONSE_CHAR_LIMIT) # Fallback
                else:
                    # No truncation: Use full content
                    if is_no_rp or is_analysis:
                        processed_content = raw_content # Use raw content directly
                    else:
                        # Use formatted parsed response without truncation for standard/drafting
                        try:
                            if assistant_response_index >= len(self.parsed_responses):
                                raise IndexError(f"Assistant response index {assistant_response_index} out of bounds for parsed_responses (len {len(self.parsed_responses)})")
                            parsed = self.parsed_responses[assistant_response_index]
                            if not isinstance(parsed, dict):
                                 logging.warning(f"Parsed response at index {assistant_response_index} is not a dict for task {self.scenario_id}. Using raw.")
                                 processed_content = raw_content
                            else:
                                processed_content = ScenarioTask._format_parsed_response(parsed) # Format without truncation
                        except IndexError as e:
                            logging.error(f"{e} accessing parsed_responses for task {self.scenario_id} (Iter {self.iteration_index}). Using raw content.", exc_info=True)
                            processed_content = raw_content # Fallback
                        except Exception as e:
                            logging.error(f"Error formatting parsed response {assistant_response_index} for task {self.scenario_id} (Iter {self.iteration_index}): {e}. Using raw content.", exc_info=True)
                            processed_content = raw_content # Fallback


                assistant_response_index += 1 # Increment only after processing an assistant message

            transcript_parts.append(f"{role_label}:\n{processed_content}\n")

        full_transcript = "---\n".join(transcript_parts)

        # Handle debrief text (only relevant for non-analysis)
        debrief_text = ""
        if not is_analysis:
            debrief_text = self.debrief_response or "" # Use empty string if None somehow
            if truncate_for_rubric and len(debrief_text) > DEBRIEF_CHAR_LIMIT:
                debrief_text = debrief_text[:DEBRIEF_CHAR_LIMIT] + '...[truncated]'

        # Format the final prompt using the specific template
        try:
            format_args = {
                "transcript": full_transcript,
                "output_format": rubric_output_format_str
            }
            # Only add debrief if it's expected by the template (i.e., not analysis)
            if not is_analysis:
                format_args["debrief"] = debrief_text

            final_rubric_prompt = rubric_prompt_template.format(**format_args)

            #print(final_rubric_prompt)

            return final_rubric_prompt
        except KeyError as e:
             # This error means the template has a placeholder not provided in format_args
             logging.error(f"Missing key '{e}' in rubric prompt template formatting for task {self.scenario_id} (Iter {self.iteration_index}, Analysis: {is_analysis}). Provided keys: {list(format_args.keys())}")
             self.status = "error"; self.error = f"Rubric Prep Error: Invalid template key ({e})."; self.rubric_run_error = f"Invalid template key ({e})."
             return None
        except Exception as e: # Catch other formatting errors
             logging.error(f"Error formatting rubric prompt for task {self.scenario_id} (Iter {self.iteration_index}): {e}", exc_info=True)
             self.status = "error"; self.error = "Rubric Prep Error: Formatting failed."; self.rubric_run_error = f"Formatting failed: {e}"
             return None

    def process_rubric_response(self, raw_judge_response: str):
        """
        Processes the raw response from the judge model for rubric scoring.
        Parses scores, updates status, scores, errors, and end time.
        """
        self.raw_rubric_judge_text = raw_judge_response
        parsed_scores = self._parse_rubric_scores(raw_judge_response)

        if parsed_scores is not None:
            self.rubric_scores = parsed_scores
            self.status = "rubric_scored" # Final success state for rubric
            self.end_time = time.time() # Mark final completion time
            self.error = None # Clear general error if scoring succeeded
            self.rubric_run_error = None # Clear specific rubric error
            logging.debug(f"Rubric scoring processed successfully for task {self.scenario_id} (Iter {self.iteration_index})")
        else:
            # Parsing failed
            error_msg = "Rubric Scoring Error: Failed to parse scores from judge response."
            logging.error(f"{error_msg} Task: {self.scenario_id} (Iter {self.iteration_index})")
            self.status = "error"
            self.error = error_msg
            self.rubric_run_error = "Failed to parse scores"
            self.rubric_scores = None # Ensure scores are None


    def to_dict(self) -> Dict[str, Any]:
        """Serializes the task state to a dictionary."""
        return {
            "scenario_id": self.scenario_id,
            "prompts": self.prompts,
            "debrief_prompt": self.debrief_prompt, # Can be None
            "iteration_index": self.iteration_index,
            "test_model": self.model_name, # Serialize logical name into 'test_model' key
            "master_prompt_template": self.master_prompt_template, # Can be None if not used
            "status": self.status,
            "start_time": self.start_time,
            "end_time": self.end_time,
            "error": self.error,
            # Specific errors
            "scenario_run_error": self.scenario_run_error,
            "debrief_run_error": self.debrief_run_error,
            "rubric_run_error": self.rubric_run_error,
            # Data payloads
            "conversation_history": self.conversation_history,
            "parsed_responses": self.parsed_responses,
            "debrief_response": self.debrief_response, # Can be None
            "rubric_scores": self.rubric_scores, # Can be None
            "raw_rubric_judge_text": self.raw_rubric_judge_text, # Can be None
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]):
        """Deserializes a task state from a dictionary."""
        # Debrief prompt is now optional
        # 'test_model' key now holds the logical name
        required_keys = ["scenario_id", "prompts", "iteration_index", "test_model"]
        if not all(key in data for key in required_keys):
             missing = [k for k in required_keys if k not in data]
             raise ValueError(f"Missing required keys in task data dictionary: {missing}. Available keys: {list(data.keys())}")

        obj = cls(
            scenario_id=data["scenario_id"],
            prompts=data["prompts"],
            debrief_prompt=data.get("debrief_prompt"), # Use .get, allows None
            iteration_index=data.get("iteration_index", 0), # Keep default for older data
            test_model=data["test_model"], # Read logical name from 'test_model' key
            master_prompt_template=data.get("master_prompt_template"), # Use .get
        )
        obj.status = data.get("status", "initialized")
        obj.start_time = data.get("start_time")
        obj.end_time = data.get("end_time")
        obj.error = data.get("error")
        # Specific errors
        obj.scenario_run_error = data.get("scenario_run_error")
        obj.debrief_run_error = data.get("debrief_run_error")
        obj.rubric_run_error = data.get("rubric_run_error")
        # Data payloads
        obj.conversation_history = data.get("conversation_history", [])
        obj.parsed_responses = data.get("parsed_responses", [])
        obj.debrief_response = data.get("debrief_response")
        obj.rubric_scores = data.get("rubric_scores")
        obj.raw_rubric_judge_text = data.get("raw_rubric_judge_text")
        return obj

# File: ai/eqbench3/core/elo.py

# core/elo.py

import os
import logging
from typing import Dict, Any, List, Tuple, Optional, Set
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timezone # Added timezone
from collections import defaultdict
from pathlib import Path
import copy # For deep copying ELO data before modification

from ..utils.file_io import load_json_file, save_json_file
from ..utils.constants import (
    ANALYSIS_SCENARIO_IDS, # Used directly
    STANDARD_PAIRWISE_PROMPT_FILE, # Used directly
    ANALYSIS_PAIRWISE_PROMPT_FILE, # Used directly
    STANDARD_SCENARIO_NOTES_FILE, # Used directly
    ANALYSIS_SCENARIO_NOTES_FILE # Used directly
)

# Import from new local modules
from .elo_config import (
    DEFAULT_ELO,
    SAMPLING_SCHEDULE,
    MAX_STAGE_LOOPS,
    WIN_MARGIN_BIN_SIZE,
    WIN_MARGIN_BIN_SIZE_FOR_CI,
    RANK_WINDOW,
    scenario_notes, # Import the global variable
    analysis_scenario_notes # Import the global variable
)
from .elo_helpers import (
    load_scenario_notes,
    should_ignore_scenario
)
from .pairwise_judging import (
    _judge_scenario_pairs_in_parallel,
    _recompute_comparison_stats
)
from .trueskill_solver import (
    solve_with_trueskill,
    normalize_elo_scores
)
from .matchup_selection import (
    build_existing_matchup_set,
    update_existing_matchups_from_comparisons, # Keep for updating the set in memory
    _pick_matchups,
    create_matchup_signature # Import needed for filtering new comparisons
)

# ─────────── Comparison-filter helpers (shared) ─────────────────────────
def _is_valid_comp(c: Dict[str, Any]) -> bool:
    """Return True if *c* is usable by the solver."""
    return (
        "error" not in c
        and not should_ignore_scenario(c.get("scenario_id"))
        and c.get("pair", {}).get("test_model")
        and c.get("pair", {}).get("neighbor_model")
    )


def filter_comparisons_for_solver(comps: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """Basic validity filter (ignores rank window)."""
    return [c for c in comps if _is_valid_comp(c)]


def filter_comps_within_rank_window(
    comps: List[Dict[str, Any]],
    elo_snapshot: Dict[str, float],
    window: int,
) -> List[Dict[str, Any]]:
    """
    Keep only comps where the two models are ≤ *window* ladder positions apart.
    *elo_snapshot* is a dict {model: rating}.
    """
    ladder = sorted(elo_snapshot, key=elo_snapshot.get)        # lowest → highest
    pos = {m: i for i, m in enumerate(ladder)}

    def _ok(pair: Dict[str, Any]) -> bool:
        a, b = pair.get("test_model"), pair.get("neighbor_model")
        return (a in pos and b in pos and abs(pos[a] - pos[b]) <= window)

    return [c for c in comps if _ok(c.get("pair", {}))]


def get_solver_comparisons(
    comps: List[Dict[str, Any]],
    elo_snapshot: Optional[Dict[str, float]] = None,
    rank_window: Optional[int] = None,
) -> List[Dict[str, Any]]:
    """
    1. Applies the basic validity filter.
    2. If *rank_window* is given, applies the ±window filter using *elo_snapshot*.
    """
    valid = filter_comparisons_for_solver(comps)
    if rank_window is not None and elo_snapshot is not None:
        return filter_comps_within_rank_window(valid, elo_snapshot, rank_window)
    return valid


def models_in_comparisons(comps: List[Dict[str, Any]]) -> Set[str]:
    """Return the set of logical model names present in *comps*."""
    mods: Set[str] = set()
    for c in comps:
        p = c.get("pair", {})
        if p.get("test_model"):    mods.add(p["test_model"])
        if p.get("neighbor_model"): mods.add(p["neighbor_model"])
    return mods
# ────────────────────────────────────────────────────────────────────────



##############################################
# Main ELO Analysis Function
##############################################

def run_elo_analysis_eqbench3(
        run_key: str,
        # File Paths
        leaderboard_elo_file: str,
        local_elo_file: str,
        # Run Data
        merged_runs_data: Dict[str, Any], # Merged leaderboard + local runs
        # Models
        test_model: str, # This is the logical model_name
        judge_model: str,
        api_clients: Dict[str, Any],
        # Other Params
        scenarios_data: Dict[str, List[str]],
        concurrency: int = 4,
        recompute_existing: bool = True
) -> Tuple[Dict[str, Any], Optional[str]]: # Return final solved ratings and error message
    """
    Three‑stage ELO procedure using merged data, writing ONLY new comparisons to local ELO file.

    1. Loads leaderboard and local ELO data.
    2. Merges comparisons and ratings (local overrides leaderboard).
    3. Builds `existing_matchups` set from the merged comparisons.
    4. Runs pairwise judging for the current `test_model` against opponents selected from the merged ladder.
    5. Filters the generated comparisons to identify *only new* ones (not in the initial `existing_matchups` set).
    6. Appends *only the new* comparisons to the `local_elo_file`.
    7. Solves ratings using the *full merged* set of comparisons (leaderboard + local + new).
    8. Returns the final solved ratings snapshot and any error message.
    """
    # ────────────────────────────── SET‑UP ────────────────────────────────
    logging.info(f"[ELO] Starting analysis for '{test_model}' (logical name)")
    logging.info(f"[ELO] Leaderboard ELO: {leaderboard_elo_file}")
    logging.info(f"[ELO] Local ELO: {local_elo_file}")
    # Access global notes defined in elo_config
    global scenario_notes, analysis_scenario_notes
    elo_error_message = None # Initialize error message

    # --- Load ELO Data ---
    leaderboard_elo = load_json_file(leaderboard_elo_file) or {"__metadata__": {}}
    local_elo = load_json_file(local_elo_file) or {"__metadata__": {}}

    # Ensure metadata structure exists
    leaderboard_elo.setdefault("__metadata__", {})
    local_elo.setdefault("__metadata__", {})

    # --- Merge Comparisons ---
    leaderboard_comps = leaderboard_elo.get("__metadata__", {}).get("global_pairwise_comparisons", [])
    local_comps = local_elo.get("__metadata__", {}).get("global_pairwise_comparisons", [])
    # Combine comparisons for building the initial set and for solving
    # Duplicates might exist if a comparison is in both, handled by build_existing_matchup_set
    all_comparisons_global: List[Dict[str, Any]] = leaderboard_comps + local_comps
    logging.info(f"[ELO] Merged comparisons: {len(leaderboard_comps)} (leaderboard) + {len(local_comps)} (local) = {len(all_comparisons_global)} total (before dedupe)")

    # --- Recompute Stats (Optional) ---
    if recompute_existing and all_comparisons_global:
        changed = 0
        for comp in all_comparisons_global:
            # Ensure the comparison has the necessary structure before recomputing
            if "pair" in comp and "test_model" in comp["pair"] and "neighbor_model" in comp["pair"] and "judge_response" in comp:
                before = comp.get("fraction_for_test")
                _recompute_comparison_stats(comp) # Now imported
                if comp.get("fraction_for_test") != before:
                    changed += 1
            elif "error" not in comp:
                 logging.warning(f"[ELO] Skipping recompute for malformed comparison: {comp.get('scenario_id', 'Unknown scenario')}")

        logging.info(f"[ELO] Recomputed plus/margin stats for "
                     f"{changed}/{len(all_comparisons_global)} stored comparisons.")

    # --- Merge ELO Ratings (Local overrides Leaderboard) ---
    # Create a deep copy to avoid modifying original dicts if they are reused elsewhere
    merged_elo_ratings = copy.deepcopy(leaderboard_elo)
    # Update with local data, overwriting existing keys and adding new ones
    for key, value in local_elo.items():
        if key != "__metadata__": # Don't overwrite metadata, comparisons handled separately
            merged_elo_ratings[key] = value

    # --- Load Prompts and Notes ---
    try:
        standard_pairwise_prompt_template = Path(STANDARD_PAIRWISE_PROMPT_FILE).read_text(encoding="utf-8")
        analysis_pairwise_prompt_template = Path(ANALYSIS_PAIRWISE_PROMPT_FILE).read_text(encoding="utf-8")
        logging.info(f"Loaded standard pairwise prompt from {STANDARD_PAIRWISE_PROMPT_FILE}")
        logging.info(f"Loaded analysis pairwise prompt from {ANALYSIS_PAIRWISE_PROMPT_FILE}")
    except Exception as e:
        logging.error(f"Failed to load pairwise prompts: {e}", exc_info=True)
        elo_error_message = f"Failed to load pairwise prompts: {e}"
        return {}, elo_error_message # Return empty dict and error

    # Load scenario notes (standard and analysis) - Use the global vars after loading
    scenario_notes.update(load_scenario_notes(STANDARD_SCENARIO_NOTES_FILE))
    analysis_scenario_notes.update(load_scenario_notes(ANALYSIS_SCENARIO_NOTES_FILE))
    logging.info(f"Loaded {len(scenario_notes)} standard scenario notes.")
    logging.info(f"Loaded {len(analysis_scenario_notes)} analysis scenario notes.")


    # --- Collect Completed Tasks from Merged Run Data ---
    all_models_scenario_results = defaultdict(lambda: defaultdict(dict))
    models_found: Set[str] = set() # Stores logical model names

    # Iterate through the merged run data
    for run_blob in merged_runs_data.values():
        # Use model_name if present, fallback to test_model for older data
        model = run_blob.get("model_name", run_blob.get("test_model"))
        if not model: continue # Skip runs without a model identifier

        for iter_idx, scenemap in run_blob.get("scenario_tasks", {}).items():
            for sid, task in scenemap.items():
                is_analysis = sid in ANALYSIS_SCENARIO_IDS
                # Analysis tasks are ready for ELO after 'scenario_completed' or 'rubric_scored'
                # Standard/Drafting tasks are ready after 'completed' or 'rubric_scored'
                required_statuses = ["scenario_completed", "rubric_scored"] if is_analysis else ["completed", "rubric_scored"]

                if task.get("status") in required_statuses and task.get("conversation_history"):
                    # Check for debrief only if not analysis
                    if not is_analysis and task.get("debrief_response") is None:
                        # logging.debug(f"Skipping task {sid} iter {iter_idx} for ELO: Missing debrief for non-analysis task.")
                        continue

                    all_models_scenario_results[model][sid][iter_idx] = task
                    models_found.add(model)

    if test_model not in models_found:
        logging.warning(f"[ELO] No finished tasks suitable for ELO found for '{test_model}'.")
        models_found.add(test_model) # Add anyway to avoid errors, will get default ELO

    # --- Initial ELO Snapshot and Existing Matchups ---
    # Build snapshot from merged ratings
    elo_snapshot = {m: merged_elo_ratings.get(m, {}).get("elo", DEFAULT_ELO) for m in models_found}
    # Add models from merged_elo_ratings that might not have tasks yet
    for m in merged_elo_ratings:
        if m != "__metadata__" and m not in elo_snapshot:
            elo_snapshot[m] = merged_elo_ratings[m].get("elo", DEFAULT_ELO)
            models_found.add(m) # Ensure all models with ratings are included

    # Build set using logical names from the combined comparison list
    initial_existing_matchups = build_existing_matchup_set(all_comparisons_global) # Now imported
    logging.info(f"[ELO] Built initial existing matchup set with {len(initial_existing_matchups)} unique signatures from merged data.")


    def _solve_for_elo(comps: List[Dict[str, Any]]) -> Tuple[Dict[str, float], Dict[str, float]]:
        """ Solves ratings and returns both mu and sigma maps. """
        mods = set() # Stores logical model names
        valid_comps = []
        for c in comps:
             pair = c.get("pair")
             if isinstance(pair, dict):
                 test_m = pair.get("test_model")
                 neigh_m = pair.get("neighbor_model")
                 if test_m and neigh_m:
                     mods.add(test_m)
                     mods.add(neigh_m)
                     valid_comps.append(c)
             elif "error" in c:
                 pass

        if not mods: return {}, {} # No valid comparisons to solve

        # Use the current snapshot's ratings as starting points if available
        start_ratings = {m: elo_snapshot.get(m, DEFAULT_ELO) for m in mods}

        logging.info(
            f"[ELO-DBG] _solve_for_elo received {len(valid_comps)} comparisons "
            f"covering {len(mods)} models"
        )
        # Pass logical names to solver
        # Ensure all models being solved for have an entry in start_ratings
        full_start_ratings = {m: start_ratings.get(m, DEFAULT_ELO) for m in mods}

        # Solve using TrueSkill, return mu and sigma
        mu_map, sigma_map = solve_with_trueskill(
            list(mods),
            valid_comps,
            full_start_ratings, # Pass initial Mu values
            debug=False,
            use_fixed_initial_ratings=True, # Use current estimates as starting point
            bin_size=WIN_MARGIN_BIN_SIZE, # Default bin size
            return_sigma=True
        )
        return mu_map, sigma_map


    # ─────────────────────────── SAMPLING LOOP ────────────────────────────
    new_comparisons_generated_this_run = [] # Store only comparisons generated in this execution
    current_existing_matchups = initial_existing_matchups.copy() # Track matchups encountered during this run

    for stage_idx, (radius_tiers, samples) in enumerate(SAMPLING_SCHEDULE, start=1):
        loops, stable = 0, False
        while (
            (radius_tiers == (None,) and loops == 0)     # exactly one iteration for stage 1
            or (radius_tiers != (None,) and not stable and loops < MAX_STAGE_LOOPS)
        ):
            loops += 1
            # Ensure test_model (logical name) is in the snapshot before sorting
            if test_model not in elo_snapshot:
                elo_snapshot[test_model] = DEFAULT_ELO
                logging.warning(f"Added missing test_model '{test_model}' to ELO snapshot with default rating.")

            # Ladder contains logical model names, sorted by current ELO estimates
            ladder = sorted(list(elo_snapshot.keys()), key=lambda m: elo_snapshot.get(m, DEFAULT_ELO))
            try:
                rank_old = ladder.index(test_model)
            except ValueError:
                 logging.error(f"Test model '{test_model}' not found in ELO ladder. Aborting ELO stage.")
                 elo_error_message = f"Test model '{test_model}' not found in ELO ladder."
                 break # Exit inner while loop

            opp_idx = _pick_matchups(rank_old, len(ladder), radius_tiers, samples) # Now imported
            if not opp_idx:
                logging.debug(f"[ELO Stage {stage_idx}] No opponents picked for rank {rank_old}. Moving to next stage or finishing.")
                break # Exit inner while loop

            comps_round: List[Dict[str, Any]] = []

            # ---------- run each opponent in parallel ---------------------
            outer_workers = min(len(opp_idx), concurrency)

            def _vs_neigh(idx: int) -> List[Dict[str, Any]]:
                neigh = ladder[idx] # neigh is a logical name
                depth = abs(idx - rank_old)

                # cap logical pairs per opponent
                if radius_tiers == (None,):           # stage‑1
                    cap = 1
                else:                                 # stage‑2 / stage‑3
                    if depth == 1:
                        cap = samples
                    elif depth == 2:
                        cap = max(1, samples // 2)
                    else:
                        cap = max(1, samples // 4)

                # Pass logical names to judging function
                # Pass current_existing_matchups set to avoid re-judging within this run
                return _judge_scenario_pairs_in_parallel( # Now imported
                    test_model, # Logical name
                    neigh,      # Logical name
                    all_models_scenario_results[test_model],
                    all_models_scenario_results[neigh],
                    concurrency,                    # inner pool
                    # Pass both templates
                    standard_pairwise_prompt_template,
                    analysis_pairwise_prompt_template,
                    scenarios_data,
                    judge_model,
                    api_clients,
                    cap,                            # ✱ per‑opponent cap ✱
                    current_existing_matchups,      # Pass the set of matchups already seen/judged
                )

            with ThreadPoolExecutor(max_workers=outer_workers) as pool:
                fut_map = {pool.submit(_vs_neigh, i): i for i in opp_idx}
                for fut in as_completed(fut_map):
                    try:
                        comps = fut.result()
                        comps_round.extend(comps)
                    except Exception as e:
                        logging.error(f"[ELO] opponent job failed: {e}", exc_info=True)
                        # Optionally store an error marker?

            # ========= SUMMARY‑OF‑MATCHUPS (single block) =========================
            if comps_round:
                per_opp   = defaultdict(list)
                new_comps_count = 0
                for comp in comps_round:
                    if "error" in comp: continue
                    pair = comp["pair"]
                    opp = pair["neighbor_model"] if pair["test_model"] == test_model else pair["test_model"]
                    per_opp[opp].append(comp["scenario_id"])
                    # Check if this comparison is truly new based on signature
                    sig = create_matchup_signature(pair["test_model"], pair["neighbor_model"], comp["scenario_id"], str(pair["iteration_index"]))
                    if sig not in initial_existing_matchups: # Check against the initial set
                         new_comps_count += 1


                if per_opp:
                    block = [
                        "",
                        "================  Matchups selected this round  ================",
                        f"Stage‑{stage_idx}  •  loop {loops}",
                        f"Test model: {test_model}",
                        f"New comparisons generated: {new_comps_count // 2} logical pairs ({new_comps_count} raw)", # Divide by 2 for logical pairs
                        "---------------------------------------------------------------",
                    ]
                    for opp, scen_list in sorted(per_opp.items()):
                        uniq = sorted(set(scen_list))
                        block.append(f"{opp}  →  {len(uniq)} logical pairs")
                        # block.extend(f"   • {sid}" for sid in uniq) # Maybe too verbose
                    block.append("================================================================")
                    logging.info("\n".join(block))
            # =====================================================================

            # --- Update global state and identify NEW comparisons ---
            newly_added_signatures = update_existing_matchups_from_comparisons(comps_round, current_existing_matchups)
            logging.debug(f"[ELO] Added {newly_added_signatures} new unique matchup signatures to the in-memory set.")

            # Filter comps_round to get only the ones generated *now* (not present initially)
            new_comparisons_this_round = []
            for comp in comps_round:
                 if "error" in comp: # Include errors generated this round
                     new_comparisons_this_round.append(comp)
                     continue
                 pair = comp.get("pair")
                 if pair:
                     sig = create_matchup_signature(pair["test_model"], pair["neighbor_model"], comp["scenario_id"], str(pair.get("iteration_index")))
                     if sig not in initial_existing_matchups:
                         new_comparisons_this_round.append(comp)

            new_comparisons_generated_this_run.extend(new_comparisons_this_round)
            all_comparisons_global.extend(new_comparisons_this_round) # Add new comps to the list used for solving


            # --- Re-solve ratings (using FULL merged comparison list) -------------
            rank_window = RANK_WINDOW if stage_idx > 1 else None
            comps_for_solver = get_solver_comparisons(
                all_comparisons_global,
                elo_snapshot if rank_window else None,
                rank_window,
            )

            # Solve using the potentially filtered list
            if comps_for_solver:
                # Solver uses logical names
                new_mu_map, _ = _solve_for_elo(comps_for_solver) # Ignore sigma map for stability check
            else:
                new_mu_map = {}

            # Update the ELO snapshot for the next loop/stability check
            new_snapshot = elo_snapshot.copy()
            new_snapshot.update(new_mu_map)

            # Ensure test_model (logical name) is still present before getting index
            if test_model not in new_snapshot:
                 new_snapshot[test_model] = DEFAULT_ELO # Add back if somehow lost
                 logging.warning(f"Test model '{test_model}' was missing from new ELO snapshot, re-added with default.")

            # Check stability based on rank change
            ladder_new = sorted(list(new_snapshot.keys()), key=lambda m: new_snapshot.get(m, DEFAULT_ELO))
            try:
                rank_new = ladder_new.index(test_model)
            except ValueError:
                 logging.error(f"Test model '{test_model}' not found in *new* ELO ladder. Stability check failed.")
                 rank_new = -1 # Indicate error
                 elo_error_message = f"Test model '{test_model}' lost during ELO update."

            stable = (rank_new == rank_old) and (rank_new != -1)
            elo_snapshot = new_snapshot # Update snapshot for the next iteration

        # End of while loop for stage
        logging.info('-------------------------------------------------')
        logging.info(
            f"[ELO] stage‑{stage_idx} finished "
            f"(elo={elo_snapshot.get(test_model, 'N/A'):.1f}, "
            f"reason={'stable rank' if stable else 'max loops reached'})"
        )
        logging.info('-------------------------------------------------')
        if rank_old == -1 or elo_error_message: # Break outer loop if test_model vanished or error occurred
            break


        # ────────────────── SAVE NEW COMPARISONS TO LOCAL ELO FILE ───────────────
    # (Save only the comparisons generated in this specific run)
    if new_comparisons_generated_this_run:
        logging.info(f"[ELO] Appending {len(new_comparisons_generated_this_run)} new comparison results to local ELO file: {local_elo_file}")
        # Load the local file again to minimize race conditions
        current_local_elo_for_comps = load_json_file(local_elo_file) or {"__metadata__": {}}
        current_local_elo_for_comps.setdefault("__metadata__", {}).setdefault("global_pairwise_comparisons", [])

        # Append only the new comparisons
        current_local_elo_for_comps["__metadata__"]["global_pairwise_comparisons"].extend(new_comparisons_generated_this_run)
        # Update timestamp only if saving comparisons succeeds
        # current_local_elo_for_comps["__metadata__"]["last_updated"] = datetime.now(timezone.utc).isoformat() # Moved timestamp update to after ratings save

        # Save back to the local file
        save_comps_success = save_json_file(current_local_elo_for_comps, local_elo_file)
        if not save_comps_success:
            logging.error(f"[ELO] FAILED to save new comparisons to {local_elo_file}")
            if not elo_error_message: # Don't overwrite existing error
                 elo_error_message = f"Failed to save new comparisons to {local_elo_file}"
        else:
             logging.info(f"[ELO] Successfully appended new comparisons to {local_elo_file}")
    else:
        logging.info("[ELO] No new comparisons were generated in this run to save.")


    # ────────────────── FINAL SOLVE & NORMALIZE (using all comps) ───────────
    logging.info("[ELO] Performing final rating calculation")
    final_snapshot = {} # This will hold the results like {"model_name": {"elo": ..., "elo_norm": ...}}
    # Use the latest elo_snapshot from the sampling loop as a fallback if solve fails
    fallback_snapshot = elo_snapshot

    try:
        # Filter out ignored scenarios and errors for the final solve
        # Filter out ignored scenarios/errors **and** apply the rank window
        final_comps_for_solver = get_solver_comparisons(
            all_comparisons_global,
            elo_snapshot,          # current ladder snapshot built earlier
            rank_window=rank_window
        )

        if final_comps_for_solver:
            # Determine the set of all models involved in valid comparisons for the final solve
            models_in_final_solve = models_in_comparisons(final_comps_for_solver)


            # Ensure all models found during task collection are included, even if they had no comparisons
            models_to_solve_for = models_found.union(models_in_final_solve)
            logging.info(f"[ELO] Final solve includes {len(models_to_solve_for)} models.")

            # Solve using the full comparison list to get final ratings
            # Use fixed initial ratings (DEFAULT_ELO) for this final solve for consistency
            final_mu_map, _ = solve_with_trueskill(
                list(models_to_solve_for), # Use the combined set of models
                final_comps_for_solver,
                {m: DEFAULT_ELO for m in models_to_solve_for}, # Start fresh from default
                debug=False,
                use_fixed_initial_ratings=True,
                bin_size=WIN_MARGIN_BIN_SIZE,
                return_sigma=True
            )

            # solve again just for sigma (from which CI is calculated)
            # using a smaller bin size so we are more fairly factoring in
            # the increase in certainty afforded by the win margin
            #  (note: this is just a guesstimate since we're already abusing
            #   trueskill's sigma estimate by expanding win margin into extra wins)
            _, final_sigma_map = solve_with_trueskill(
                list(models_to_solve_for), # Use the combined set of models
                final_comps_for_solver,
                {m: DEFAULT_ELO for m in models_to_solve_for}, # Start fresh from default
                debug=False,
                use_fixed_initial_ratings=True,
                bin_size=WIN_MARGIN_BIN_SIZE_FOR_CI, # Use bin_size=5 for CI calculation
                return_sigma=True
            )

            # Normalize scores based on the solved Mu values
            normalized_scores = normalize_elo_scores(final_mu_map)

            # Combine results into the final snapshot structure
            # Use models_to_solve_for to ensure all relevant models get an entry
            ts_env_sigma = 350/3 # Default sigma from TrueSkill setup
            for m in models_to_solve_for:
                mu_raw = final_mu_map.get(m, DEFAULT_ELO)
                # Use solved sigma if available, otherwise default env sigma
                sigma = final_sigma_map.get(m, ts_env_sigma)
                mu_norm = normalized_scores.get(m, DEFAULT_ELO) # Use default if normalization failed

                # Calculate CI bounds on raw score
                ci_low_raw = mu_raw - 1.96 * sigma
                ci_high_raw = mu_raw + 1.96 * sigma

                # Store everything
                final_snapshot[m] = {
                    "elo": round(mu_raw, 2),
                    "elo_norm": round(mu_norm, 2),
                    "sigma": round(sigma, 2),
                    "ci_low": round(ci_low_raw, 2),
                    "ci_high": round(ci_high_raw, 2),
                    # We need normalized CI bounds too
                }

            # --- Normalize CI bounds ---
            # Create a dict with raw scores + bounds for normalization
            raw_plus_bounds = {}
            for m, data in final_snapshot.items():
                raw_plus_bounds[m] = data["elo"]
                raw_plus_bounds[f"{m}__low"] = data["ci_low"]
                raw_plus_bounds[f"{m}__high"] = data["ci_high"]

            norm_plus = normalize_elo_scores(raw_plus_bounds)

            # Add normalized bounds back to the final snapshot
            for m in final_snapshot:
                 # Use normalized mu as fallback if bounds missing
                 norm_mu_fallback = final_snapshot[m]["elo_norm"]
                 final_snapshot[m]["ci_low_norm"] = round(norm_plus.get(f"{m}__low", norm_mu_fallback), 2)
                 final_snapshot[m]["ci_high_norm"] = round(norm_plus.get(f"{m}__high", norm_mu_fallback), 2)

            logging.info("[ELO] Final rating calculation and normalization complete.")

        else:
            logging.warning("[ELO] No valid comparisons available for final solve.")
            if not elo_error_message: elo_error_message = "No valid comparisons for final solve"
            # If solve didn't run, create a basic snapshot from the fallback
            final_snapshot = {m: {"elo": r, "elo_norm": r} for m, r in fallback_snapshot.items()}

    except Exception as e:
        logging.error(f"[ELO] Final solve or normalization failed: {e}", exc_info=True)
        if not elo_error_message: elo_error_message = f"Final solve/normalization failed: {e}"
        # If solve failed, create a basic snapshot from the fallback
        final_snapshot = {m: {"elo": r, "elo_norm": r} for m, r in fallback_snapshot.items()}


    # ────────────────── SAVE FINAL RATINGS TO LOCAL ELO FILE ───────────────
    # Overwrite top-level model keys with the newly calculated ratings
    logging.info(f"[ELO] Saving final ratings snapshot to local ELO file: {local_elo_file}")
    try:
        # Load the local file again to ensure we have the latest comparisons list
        current_local_elo = load_json_file(local_elo_file) or {"__metadata__": {}}
        current_local_elo.setdefault("__metadata__", {}) # Ensure metadata key exists

        # Update the top-level model entries with the latest solved ratings
        for model_name_key, rating_data in final_snapshot.items():
            if model_name_key != "__metadata__": # Prevent accidentally overwriting metadata
                current_local_elo[model_name_key] = rating_data

        # Update timestamp
        current_local_elo["__metadata__"]["last_updated"] = datetime.now(timezone.utc).isoformat()

        # Save the updated structure back to the local file
        save_ratings_success = save_json_file(current_local_elo, local_elo_file)
        if not save_ratings_success:
            logging.error(f"[ELO] FAILED to save final ratings to {local_elo_file}")
            if not elo_error_message: # Don't overwrite existing error
                 elo_error_message = f"Failed to save final ratings to {local_elo_file}"
        else:
             logging.info(f"[ELO] Successfully saved final ratings to {local_elo_file}")

    except Exception as e:
         logging.error(f"[ELO] Error saving final ratings to {local_elo_file}: {e}", exc_info=True)
         if not elo_error_message:
              elo_error_message = f"Error saving final ratings: {e}"


    # Return the computed ratings snapshot and any error message
    return final_snapshot, elo_error_message
